import os
from contextlib import asynccontextmanager
from dotenv import load_dotenv
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from fastapi.responses import StreamingResponse

load_dotenv()
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'

from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.core import Settings, VectorStoreIndex, PropertyGraphIndex
from llama_index.llms.dashscope import DashScope
from llama_index.vector_stores.qdrant import QdrantVectorStore
from llama_index.graph_stores.neo4j import Neo4jPropertyGraphStore
from llama_index.core.llms import ChatMessage
from llama_index.core.retrievers import VectorContextRetriever
from llama_index.core.memory import ChatMemoryBuffer
from llama_index.core.chat_engine import ContextChatEngine # è¿™ä¸ªç±»åå‘äºåº•å±‚ï¼Œè€Œas_chat_engineæ˜¯å°è£…å®Œæ¯•çš„é«˜çº§ä¸€äº›çš„æ¥å£ï¼Œåº•å±‚ç”¨çš„ä»ç„¶æ˜¯ContextChatEngine
from llama_index.core.chat_engine.types import ChatMode
import qdrant_client

rag_engine = {}
QDRANT_URL = "http://localhost:6333"
QDRANT_COLLECTION = "edu_matrix_v2"
NEO4J_URL = "bolt://localhost:7687"
NEO4J_USER = "neo4j"
NEO4J_PASSWORD = "password123"

@asynccontextmanager
async def lifespan(app: FastAPI):
    print("ğŸš€ [Startup] æ­£åœ¨åˆå§‹åŒ– EduMatrix å¼•æ“...")
    
    try:
        # 1. åˆå§‹åŒ– Embedding æ¨¡å‹ (æœ¬åœ°)
        print("ğŸ§  åŠ è½½ Embedding æ¨¡å‹...")
        embed_model = HuggingFaceEmbedding( # from llama_index.embeddings.huggingface import ...
            model_name="BAAI/bge-m3",
            trust_remote_code=True,
            local_files_only=True,
            device="cpu",
        )
        Settings.embed_model = embed_model

        # 2. åˆå§‹åŒ– LLM (é˜¿é‡Œäº‘ Qwen)
        print("ğŸ¤– è¿æ¥ DashScope LLM...")
        llm = DashScope( # from llama_index.llms.dashscope import ...
            model_name=os.getenv("DASHSCOPE_MODEL_NAME"),
            api_key=os.getenv("DASHSCOPE_API_KEY"),
            temperature=0.1,
        )

        # 3. è¿æ¥å‘é‡æ•°æ®åº“ (Qdrant)
        print("ğŸ”Œ è¿æ¥ Qdrant...")
        client = qdrant_client.QdrantClient( # è¿™æ˜¯qdrantçš„åŸç”Ÿåº“ï¼Œè¿æ¥qdrantå®¢æˆ·ç«¯
            url=QDRANT_URL,
        )
        vector_store = QdrantVectorStore( # è®©llama-indexèƒ½å¤Ÿä½¿ç”¨qdrantï¼ŒæŠŠllamaindexçš„æŒ‡ä»¤ç¿»è¯‘æˆqdrantèƒ½å¬æ‡‚çš„åŸç”ŸæŒ‡ä»¤
            collection_name=QDRANT_COLLECTION,
            client=client,
        ) # ä¸ä»…è´Ÿè´£å­˜å‚¨ï¼Œè¿˜è´Ÿè´£æ‰¾æ•°æ®

        # index:æ˜¯é™æ€çš„ï¼Œä¸è´Ÿè´£æŸ¥è¯¢æ£€ç´¢åŠ¨ä½œï¼Œè€Œæ˜¯è´Ÿè´£ç»´æŠ¤æ•°æ®ç»“æ„
        #       å¦‚æœæ˜¯from_documentsï¼šè´Ÿè´£æŠŠä¹±ä¸ƒå…«ç³Ÿçš„æ–‡æ¡£æ•´ç†æˆæœ‰åºçš„å‘é‡æˆ–å›¾è°±ï¼ˆå»ºæ•°æ®åº“ï¼‰
        #       å¦‚æœæ˜¯from_existingï¼šä»£è¡¨æ•´ä¸ªæ•°æ®åº“çš„è®¿é—®å¥æŸ„ï¼ŒçŸ¥é“æ•°æ®åº“ä¸­æœ‰å•¥å†…å®¹ã€‚
        #       from_vector_storeåŸç†ä¸from_existingæ˜¯ä¸€æ ·çš„ã€‚
        # ä¸retrieverçš„å…³ç³»ï¼šå®ƒæ‰‹é‡Œæ¡ç€æ•°æ®åº“è¿æ¥ï¼Œå®ƒèƒ½ç”Ÿäº§å‡ºå„ç§å„æ ·çš„å·¥å…·ï¼Œå…¶ä¸­ä¸€ä¸ªå·¥å…·å°±æ˜¯ Retrieverã€‚

        vector_index = VectorStoreIndex.from_vector_store(
            vector_store=vector_store,
        )

        # 4. è¿æ¥å›¾æ•°æ®åº“ (Neo4j)
        print("ğŸ”Œ è¿æ¥ Neo4j...") # property graphï¼šå¸¦å±æ€§çš„å›¾
        graph_store = Neo4jPropertyGraphStore( # ç‰©ç†å­˜å‚¨
            username=NEO4J_USER,
            password=NEO4J_PASSWORD,
            url=NEO4J_URL,
        )
        graph_index = PropertyGraphIndex.from_existing( # .from_existingï¼šè¯»å–æ¨¡å¼ï¼Œä»ç°æœ‰çš„æ•°æ®åº“ä¸­è¯»å–
            property_graph_store=graph_store,
            # embed_model=embed_model,
            llm=llm
        ) # .from_documentsï¼šä»æ–‡æ¡£åˆ›å»ºï¼Œè¿›è¡Œå‘é‡åŒ–ä¸å›¾è°±åŒ–

        # rag_engine["vector_retriever"] = vector_index.as_retriever(similarity_top_k=4)

        sub_retriever = VectorContextRetriever( # è¿™ä¸€æ­¥åŒ…å«æŠŠç”¨æˆ·é—®é¢˜è½¬æˆå‘é‡ï¼Œå»neo4jä¸­æŸ¥è¯¢å‘é‡ç›¸ä¼¼çš„èŠ‚ç‚¹ï¼Œç„¶åæŠ“å–å­å›¾
            graph_store=graph_store,
            # embed_model=embed_model,
            similarity_top_k=5,
            path_depth=2 # å¤šè·³
        )

        # rag_engine["graph_retriever"] = graph_index.as_retriever(
        #     sub_retrievers=[sub_retriever,],
        #     include_text=True,
        # )

        # rag_engine["llm"] = llm

        vector_tool = vector_index.as_retriever(similarity_top_k=5)
        graph_tool = graph_index.as_retriever(
            sub_retrievers=[sub_retriever,]
        )

        from llama_index.core.retrievers import BaseRetriever

        class HybridRetriever(BaseRetriever):
            def __init__(self, vector_retriever, graph_retriever):
                self.vector_retriever = vector_retriever
                self.graph_retriever = graph_retriever
                super().__init__()
            
            def _retrieve(self, query_bundle):
                nodes_vect = self.vector_retriever.retrieve(query_bundle)
                nodes_graph = self.graph_retriever.retrieve(query_bundle)

                return list({n.node.node_id: n for n in (nodes_vect + nodes_graph)}.values())
        
        hybrid_retriever = HybridRetriever(vector_tool, graph_tool)

        # å®šä¹‰å†…å­˜ç¼“å†²åŒº
        memory = ChatMemoryBuffer.from_defaults(token_limit=3000)

        # è¿™é‡Œæˆ‘ä»¬ç”¨ ContextChatEngineï¼Œé…åˆæˆ‘ä»¬çš„æ··åˆæ£€ç´¢å™¨
        chat_engine = ContextChatEngine.from_defaults(
            retriever=hybrid_retriever,
            memory=memory,
            llm=llm,
            system_prompt="""
            ä½ æ˜¯ä¸€åä¸“ä¸šçš„è®¡ç®—æœºè¯¾ç¨‹åŠ©æ•™ (EduMatrix)ã€‚
            
            ã€ä½ çš„èƒ½åŠ›ã€‘ï¼š
            1. ä½ æ‹¥æœ‰ã€å¯¹è¯å†å²ã€‘ï¼ˆç”¨æˆ·å’Œä½ ä¹‹å‰çš„èŠå¤©è®°å½•ï¼‰ã€‚
            2. ä½ æ‹¥æœ‰ã€èƒŒæ™¯çŸ¥è¯†ã€‘ï¼ˆæ£€ç´¢åˆ°çš„æ•™æåŸæ–‡å’Œå›¾è°±å…³ç³»ï¼‰ã€‚

            ã€å›ç­”ç­–ç•¥ã€‘ï¼š
            1. ğŸš¨ **æœ€é«˜ä¼˜å…ˆçº§**ï¼šå¦‚æœç”¨æˆ·çš„é—®é¢˜æ˜¯å…³äº**â€œä½ åˆšæ‰è¯´çš„â€ã€â€œä¸Šä¸€æ¬¡å›ç­”â€**æˆ–**â€œä¹‹å‰çš„å¯¹è¯â€**ï¼ˆä¾‹å¦‚ï¼šâ€œä½ åˆšåˆšåˆ—å‡ºçš„ç¬¬ä¸€ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿâ€ï¼‰ï¼Œè¯·**åŠ¡å¿…ä¼˜å…ˆåŸºäºã€å¯¹è¯å†å²ã€‘**è¿›è¡Œå›ç­”ï¼Œå¿½ç•¥ä¸ä¹‹å†²çªçš„æ£€ç´¢ç»“æœã€‚
            2. å¯¹äºå…¶ä»–çŸ¥è¯†æ€§é—®é¢˜ï¼ˆä¾‹å¦‚ï¼šâ€œä»€ä¹ˆæ˜¯Word2Vecï¼Ÿâ€ï¼‰ï¼Œè¯·åŸºäºæ£€ç´¢åˆ°çš„ã€èƒŒæ™¯çŸ¥è¯†ã€‘å›ç­”ã€‚
            3. ç»“åˆåŸæ–‡å’Œå›¾è°±å…³ç³»ï¼Œä½¿å¾—å›ç­”æ—¢å‡†ç¡®åˆæœ‰é€»è¾‘ã€‚
            """,
        )

        # system_prompt = """
        #     ä½ æ˜¯ä¸€åä¸“ä¸šçš„è®¡ç®—æœºè¯¾ç¨‹åŠ©æ•™ (EduMatrix)ã€‚
        #     ä½ çš„çŸ¥è¯†åº“åŒ…å«äº†æ•™æåŸæ–‡ï¼ˆå‘é‡ï¼‰å’Œæ¦‚å¿µå…³ç³»å›¾è°±ï¼ˆç»“æ„åŒ–çŸ¥è¯†ï¼‰ã€‚
            
        #     è¯·ç»¼åˆåˆ©ç”¨æ£€ç´¢åˆ°çš„ã€èƒŒæ™¯çŸ¥è¯†ã€‘å›ç­”ç”¨æˆ·é—®é¢˜ã€‚
        #     å›ç­”æ—¶ï¼Œè¯·å°è¯•ç†æ¸…æ¦‚å¿µä¹‹é—´çš„å…³ç³»ï¼ˆä¾‹å¦‚ï¼šAæ˜¯Bçš„ç»„æˆéƒ¨åˆ†ï¼ŒCå¯¼è‡´äº†Dï¼‰ã€‚
        #     å¦‚æœèƒŒæ™¯çŸ¥è¯†ä¸è¶³ï¼Œè¯·è¯šå®åœ°è¯´ä¸çŸ¥é“ã€‚
        #     """

        # ä»€ä¹ˆæ˜¯ChatEngine?å®ƒæ˜¯LlamaIndexçš„é«˜çº§å°è£…ã€‚å·¥ä½œæµç¨‹ï¼šç”¨æˆ·æé—® -> çœ‹å†å²è®°å½• -> é‡å†™é—®é¢˜ -> å»å‘é‡åº“æ£€ç´¢ -> ç»™LLM
        # chat_engine = vector_index.as_chat_engine( # ä½¿ç”¨å¯¹è¯å¼•æ“ï¼Œè®°è½½å†å²å¯¹è¯
        #     chat_mode=ChatMode.CONTEXT, # æ¯æ¬¡å›ç­”ï¼Œéƒ½ä¼šå…ˆå»æ£€ç´¢ç›¸å…³æ–‡æ¡£
        #     memory=memory,
        #     system_prompt=system_prompt,
        #     llm=llm,
        #     similarity_top_k=5,
        # )

        # chat_engine = graph_index.as_chat_engine(
        #     chat_mode=ChatMode.CONTEXT,
        #     memory=memory,
        #     llm=llm,
        #     system_prompt=system_prompt,
        #     retriever_kwargs={
        #         "sub_retrievers": [sub_retriever],
        #         "include_text": True,
        #     }
        # )

        rag_engine["chat_engine"] = chat_engine

        print("âœ… å¼•æ“åˆå§‹åŒ–å®Œæˆï¼ç­‰å¾…è¯·æ±‚...")

    except Exception as e:
        print(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
        raise e

    yield # æœåŠ¡å™¨å¼€å§‹è¿è¡Œ

    # æœåŠ¡å™¨å…³é—­æ—¶çš„æ¸…ç†å·¥ä½œ (è¿™é‡Œæš‚æ—¶ä¸éœ€è¦)
    print("ğŸ‘‹ [Shutdown] æœåŠ¡å™¨å·²å…³é—­")

app = FastAPI(title="EduMatrix API", lifespan=lifespan)


# class ChatRequest(BaseModel):
#     query: str

# class ChatResponse(BaseModel):
#     answer: str
#     sources: list[str]

class Message(BaseModel):
    role: str
    content: str

class ChatRequest(BaseModel):
    messages: list[Message]


# @app.post("/api/chat")
# async def chat_endpoint(request: ChatRequest):
#     if not rag_engine:
#         raise HTTPException(status_code=500, detail="Engine not initialized")
    
#     query = request.query
#     print(f"ğŸ“© æ”¶åˆ°æé—®: {query}")

#     # 1. æ‰§è¡Œæ··åˆæ£€ç´¢
#     vector_nodes = rag_engine["vector_retriever"].retrieve(query)
#     graph_nodes = rag_engine["graph_retriever"].retrieve(query)

#     # 2. æ•´ç†ä¸Šä¸‹æ–‡
#     all_nodes = vector_nodes + graph_nodes
#     context_str = "\n".join([n.text for n in all_nodes])
    
#     # æ”¶é›†æ¥æºä¿¡æ¯ (ä¸ºäº†å±•ç¤ºç»™ç”¨æˆ·çœ‹)
#     source_texts = [n.text[:150].replace('\n', ' ') + "..." for n in all_nodes]

#     # 3. æ„é€  Prompt
#     prompt = f"""
#     ä½ æ˜¯ä¸€åä¸“ä¸šçš„è®¡ç®—æœºè¯¾ç¨‹åŠ©æ•™ã€‚è¯·åŸºäºä»¥ä¸‹ã€èƒŒæ™¯çŸ¥è¯†ã€‘å›ç­”ç”¨æˆ·çš„ã€é—®é¢˜ã€‘ã€‚
#     å¦‚æœèƒŒæ™¯çŸ¥è¯†ä¸è¶³ä»¥å›ç­”ï¼Œè¯·è¯šå®åœ°è¯´ä¸çŸ¥é“ï¼Œä¸è¦ç¼–é€ ã€‚

#     ã€é—®é¢˜ã€‘ï¼š{query}

#     ã€èƒŒæ™¯çŸ¥è¯†ã€‘ï¼š
#     {context_str}
#     """

#     # 4. è°ƒç”¨ LLM
#     # DashScope çš„ chat æ¥å£
#     messages = [
#         ChatMessage(role="system", content="ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„åŠ©æ•™ã€‚"),
#         ChatMessage(role="user", content=prompt)
#     ]
#     streaming_response = rag_engine["llm"].stream_chat(messages)

#     def response_generator():
#         # A. å…ˆæŠŠ LLM ç”Ÿæˆçš„ç­”æ¡ˆï¼Œä¸€ä¸ªå­—ä¸€ä¸ªå­—åç»™å‰ç«¯
#         for token in streaming_response:
#             # 1. å°è¯•å–å¢é‡æ–‡æœ¬ (Standard LlamaIndex streaming)
#             if hasattr(token, 'delta') and token.delta:
#                 yield token.delta
#             # 2. å¦‚æœæ²¡æœ‰ deltaï¼Œå°è¯•å– message.content
#             elif hasattr(token, 'message') and token.message.content:
#                 yield token.message.content
#             # 3. å¦‚æœå®ƒæœ¬èº«å°±æ˜¯å­—ç¬¦ä¸² (é˜²å¾¡æ€§ç¼–ç¨‹)
#             elif isinstance(token, str):
#                 yield token
#             # 4. å®åœ¨ä¸è¡Œï¼Œå¼ºè½¬å­—ç¬¦ä¸² (è™½ç„¶å¯èƒ½ä¼šå¸¦ä¸Šæ ¼å¼å™ªéŸ³ï¼Œä½†è‡³å°‘ä¸æŠ¥é”™)
#             else:
#                 yield str(token)

#         # B. ç­”æ¡ˆåå®Œäº†ï¼Œæˆ‘ä»¬åœ¨æœ€åè¿½åŠ "å‚è€ƒæ¥æº"
#         # è¿™æ ·ç”¨æˆ·æœ€åèƒ½çœ‹åˆ°å¼•ç”¨äº†å“ªäº›ä¹¦
#         if source_texts:
#             yield "\n\n---\n**ğŸ“š å‚è€ƒæ¥æºï¼š**\n"
#             for src in source_texts:
#                 yield f"- {src}\n"
    
#     return StreamingResponse(response_generator(), media_type="text/plain")

@app.post("/api/chat")
async def chat_endpoint(request: ChatRequest):
    if not rag_engine:
        raise HTTPException(status_code=500, detail="Engine not initialized")
    
    # è·å–ç”¨æˆ·æçš„é—®é¢˜
    last_message = request.messages[-1].content
    print(f"ğŸ“© æ”¶åˆ°é—®é¢˜: {last_message}")

    # å°†å‰©ä½™çš„æ¶ˆæ¯å……å½“å†å²æ¶ˆæ¯
    chat_history = [
        ChatMessage(role=m.role, content=m.content)
        for m in request.messages[:-1] # å»é™¤æœ€åä¸€æ¡æ¶ˆæ¯
    ]

    # è°ƒç”¨å¼•æ“
    streaming_response = rag_engine["chat_engine"].stream_chat(
        last_message,
        chat_history=chat_history,
    )

    # è¾“å‡º
    def response_generator():
        for token in streaming_response.response_gen: # æœç´¢å¼•æ“çš„æµå¼è¾“å‡ºæ‰æœ‰response_genï¼Œè€ŒåŸæ¥çš„æ£€ç´¢å™¨æ£€ç´¢ï¼ˆas_retrieverï¼‰æ£€ç´¢æ—¶ç›´æ¥ç”¨stream_responseå°±è¡Œäº†
            yield token
        
        if streaming_response.source_nodes:
            yield "\n\n---\n**ğŸ“š å‚è€ƒæ¥æºï¼š**\n"
            for node in streaming_response.source_nodes:
                # è¿™é‡Œçš„ node.text å°±æ˜¯æ£€ç´¢åˆ°çš„æ•™æç‰‡æ®µ
                clean_text = node.text[:100].replace('\n', ' ')
                yield f"- {clean_text}...\n"
            
    return StreamingResponse(response_generator(), media_type="text/plain")

@app.get("/")
def read_root():
    return {"message": "EduMatrix API is running! Go to /docs for Swagger UI."}