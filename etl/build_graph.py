import os
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'

from dotenv import load_dotenv
from pathlib import Path
import re

from llama_index.llms.dashscope import DashScope
from llama_index.core import Settings, PropertyGraphIndex, SimpleDirectoryReader
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.graph_stores.neo4j import Neo4jPropertyGraphStore
from llama_index.core.indices.property_graph import SimpleLLMPathExtractor # from llama_index.core.indices.property_graph import SchemaLLMPathExtractor åƒé—®ä¸openaiæ ¼å¼ä¸åŒï¼Œä¸å…¼å®¹
from llama_parse import LlamaParse, ResultType
load_dotenv()

def clean_text(text: str) -> str:
    """æ¸…é™¤PDFæå–æ–‡æœ¬ä¸­çš„å™ªå£°"""
    # å»æ‰é¡µç 
    text = re.sub(r'^\s*\d+\s*$', ' ', text, flags=re.MULTILINE)

    text = text.replace('æ·±åº¦å­¦ä¹ è¿›é˜¶ï¼šè‡ªç„¶è¯­è¨€å¤„ç†', '')

    text = re.sub(r'\n\s*\n', '\n', text)
    
    return text.strip()

# 1. å®šä¹‰ä¸€ä¸ªè‡ªå®šä¹‰çš„è§£æå‡½æ•°ï¼Œä¸“é—¨å¤„ç† "å®ä½“ | å…³ç³» | å®ä½“" è¿™ç§æ ¼å¼
def custom_parse_triplets(llm_output: str):
    """
    æ‰‹åŠ¨è§£æ LLM è¾“å‡ºï¼Œé¿å…é€—å·å¹²æ‰°ã€‚
    æœŸæœ›æ ¼å¼: å®ä½“1 | å…³ç³» | å®ä½“2
    """
    triplets = []
    lines = llm_output.strip().split("\n")
    for line in lines:
        # è·³è¿‡ç©ºè¡Œæˆ–è¿‡çŸ­çš„è¡Œ
        if len(line) < 5: 
            continue
            
        # ä½¿ç”¨ | è¿›è¡Œåˆ‡åˆ†
        parts = line.split("|")
        if len(parts) == 3:
            subj = parts[0].strip()
            pred = parts[1].strip()
            obj = parts[2].strip()
            
            # ğŸ§¹ æ•°æ®æ¸…æ´—ï¼šå¦‚æœå®ä½“æ˜¯çº¯æ•°å­—ã€å•å­—æ¯å˜é‡(x, y)ï¼Œæˆ–è€…çœ‹èµ·æ¥åƒä¹±ç ï¼Œç›´æ¥ä¸¢å¼ƒ
            # è¿™é‡Œç”¨æ­£åˆ™è¿‡æ»¤æ‰ "0", "1", "t", "(0,0,1)" è¿™ç§åƒåœ¾å®ä½“
            if len(subj) < 2 or len(obj) < 2:
                continue
            if re.match(r'^[\d\(\)\[\],.=\s]+$', subj): # è¿‡æ»¤çº¯æ•°å­—ç¬¦å·ç»„åˆ
                continue
                
            triplets.append((subj, pred, obj))
    return triplets



PDF_PATH = Path(__file__).resolve().parents[1] / "data" # / "æ·±åº¦å­¦ä¹ è¿›é˜¶_è‡ªç„¶è¯­è¨€å¤„ç†_æ–‹è—¤åº·æ¯….pdf"
# PDF_PATH = "../data/æ·±åº¦å­¦ä¹ è¿›é˜¶_è‡ªç„¶è¯­è¨€å¤„ç†_æ–‹è—¤åº·æ¯….pdf"

NEO4J_USER = "neo4j"
NEO4J_PASSWORD = "password123"
NEO4J_URI = "bolt://localhost:7687"

TEST_MODE = True

def split_markdown_semantic(md_text: str):
    """
    å°† LlamaParse è¾“å‡ºçš„ Markdown æ‹†æˆã€çº¯æ­£æ–‡ chunkã€‘
    - ä¸¢å¼ƒè¡¨æ ¼
    - ä¸¢å¼ƒè¿‡çŸ­å™ªå£°
    """
    chunks = []
    buffer = []

    for line in md_text.splitlines():
        line = line.strip()

        # ä¸¢å¼ƒè¡¨æ ¼
        if line.startswith("|") or re.match(r"^\|?[-: ]+\|?$", line):
            continue

        # æ ‡é¢˜ï¼šåˆ‡ chunk
        if line.startswith("#"):
            if buffer:
                chunk = "\n".join(buffer).strip()
                if len(chunk) > 80:
                    chunks.append(chunk)
                buffer = []
            continue

        if line:
            buffer.append(line)

    if buffer:
        chunk = "\n".join(buffer).strip()
        if len(chunk) > 80:
            chunks.append(chunk)

    return chunks

def main():
    print(f"ğŸš€ [Graph] å‡†å¤‡æ„å»ºçŸ¥è¯†å›¾è°±...")

    print(f"ğŸ¤– åˆå§‹åŒ– LLM: {os.getenv('DASHSCOPE_MODEL_NAME')}...")
    llm = DashScope(
            model_name=os.getenv("DASHSCOPE_MODEL_NAME"),
            api_key=os.getenv("DASHSCOPE_API_KEY"),
            temperature=0.1,
        )
    Settings.llm = llm

    # åˆå§‹åŒ–è§£æå™¨
    parser = LlamaParse(
        result_type=ResultType.MD, # è¾“å‡ºæ ¼å¼ä¸ºmarkdown
        verbose=True, # åœ¨ç»ˆç«¯æ‰“å°è¯¦ç»†çš„è¿›åº¦æ¡å’Œæ—¥å¿—
        language="ch_sim",
        num_workers=4,
        api_key=os.getenv("LLAMACLOUD_API_KEY") or "",
        fast_mode=False,
        system_prompt="""
        è¿™æ˜¯ä¸€ä¸ªè®¡ç®—æœºç§‘å­¦æ•™æã€‚
        1. è¯·ç²¾ç¡®ä¿ç•™æ‰€æœ‰çš„æ•°å­¦å…¬å¼ï¼ˆä½¿ç”¨ LaTeX æ ¼å¼ï¼‰ã€‚
        2. ä¸è¦è¾“å‡ºé¡µçœ‰å’Œé¡µè„šçš„é¡µç ä¿¡æ¯ã€‚
        3. å¦‚æœé‡åˆ°è¡¨æ ¼ï¼Œè¯·å°†å…¶è½¬æ¢ä¸º Markdown è¡¨æ ¼ã€‚
        4. ä¿æŒæ­£æ–‡çš„è¿è´¯æ€§ã€‚ä¸è¦è¾“å‡º 'Here are some facts' è¿™ç±»æ— å…³æ–‡å­—ã€‚
        """,
    )

    file_extractor = {".pdf": parser}

    print("â³ æ­£åœ¨è¯·æ±‚ LlamaCloud API è¿›è¡Œäº‘ç«¯è§£æï¼ˆè¿™å¯èƒ½éœ€è¦å‡ åç§’ï¼‰...")
    # documents = parser.load_data(PDF_PATH)

    # SimpleDirectoryReaderä¸­åŠ å…¥å‚æ•°recursive=Trueï¼Œå¯ä»¥è®©è¿™ä¸ªreaderè¯»å–å¡«å…¥çš„è·¯å¾„ä¸‹çš„å­æ–‡ä»¶å¤¹
    raw_docs = SimpleDirectoryReader(input_dir=PDF_PATH, file_extractor=file_extractor).load_data() # pyright: ignore[reportArgumentType]
    # for doc in documents:
    #     # è·å–åŸå§‹å†…å®¹
    #     original_text = doc.get_content() # æˆ–è€… doc.text
    
    #     # æ¸…æ´—
    #     cleaned_text = clean_text(original_text)
    
    #     # âœ… ä½¿ç”¨ set_content æ›¿ä»£ doc.text = ... ä»¥æ¶ˆé™¤ Pylance æŠ¥é”™
    #     doc.set_content(cleaned_text)

    # print(f"ğŸ§¹ å·²æ¸…æ´— {len(documents)} é¡µæ–‡æ¡£çš„å™ªå£°æ•°æ®ã€‚")
    
    """
    # âš ï¸ æµ‹è¯•æ¨¡å¼æˆªæ–­
    if TEST_MODE:
        print("âš¡ï¸ [æµ‹è¯•æ¨¡å¼] ä»…å¤„ç†å‰ 20 é¡µæ•°æ®...")
        documents = documents[30:45]
    """

    raw_docs = raw_docs[30:60]

    documents = []

    for doc in raw_docs:
        md_text = clean_text(doc.get_content())
        chunks = split_markdown_semantic(md_text)

        for chunk in chunks:
            documents.append(
                doc.__class__(
                    text=chunk,
                    metadata={
                        "source": "raw_text",
                        "type": "text"
                    }
                )
            )

    print(f"âœ… ç»“æ„åŒ–å®Œæˆï¼Œæœ€ç»ˆå…¥åº“ chunk æ•°é‡: {len(documents)}")
    
    graph_store = Neo4jPropertyGraphStore(
        username=NEO4J_USER,
        password=NEO4J_PASSWORD,
        url=NEO4J_URI
    )

    print("ğŸ§  å¼€å§‹æå–çŸ¥è¯†å®ä½“ä¸å…³ç³» (Graph Extraction)...")
    print("â˜•ï¸ è¿™æ­¥æ¯”è¾ƒæ…¢ï¼ŒQwen æ­£åœ¨é˜…è¯»å¹¶æ•´ç†çŸ¥è¯†ç‚¹ï¼Œè¯·è€å¿ƒç­‰å¾…...")

    embed_model = HuggingFaceEmbedding(
        model_name="BAAI/bge-m3",
        trust_remote_code=True,
        local_files_only=True,
    )

    # kg_extractor = SimpleLLMPathExtractor(
    #     llm=llm,
    #     max_paths_per_chunk=15, # æ¯æ®µæ–‡æœ¬æœ€å¤šæå–15æ¡å…³ç³»ï¼Œé˜²æ­¢å¹»è§‰
    #     num_workers=4
    # )

    # kg_prompt_template = (
    #     "ä½ æ˜¯ä¸€ä¸ªçŸ¥è¯†å›¾è°±æå–ä¸“å®¶ã€‚\n"
    #     "è¯·ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æå–å®ä½“å’Œå…³ç³»ï¼Œæ ¼å¼ä¸º (å®ä½“1, å…³ç³», å®ä½“2)ã€‚\n"
    #     "ä¸è¦è¾“å‡ºä»»ä½•ä»‹ç»æ€§æ–‡å­—ï¼ˆå¦‚'Here are some facts...'ï¼‰ã€‚\n"
    #     "---------------------\n"
    #     "{text}\n"
    #     "---------------------\n"
    # )

#     kg_prompt_template = """
# ä½ æ˜¯ä¸€åã€ä¸­æ–‡è®¡ç®—æœºæ•™æã€‘çŸ¥è¯†å›¾è°±æ„å»ºä¸“å®¶ã€‚

# è¯·ä»ä¸‹åˆ—æ•™ææ–‡æœ¬ä¸­ï¼Œå°½å¯èƒ½å¤šåœ°æå–ã€æœ‰æ„ä¹‰çš„å®ä½“å…³ç³»ä¸‰å…ƒç»„ã€‘ã€‚

# è¦æ±‚ï¼š
# 1. æ¯æ¡è¾“å‡ºä¸ºä¸€è¡Œ
# 2. æ ¼å¼ä¸ºï¼šå®ä½“1, å…³ç³», å®ä½“2
# 3. å®ä½“è¯·ä½¿ç”¨æ•™æä¸­çš„åŸå§‹ä¸­æ–‡æœ¯è¯­
# 4. å…³ç³»è¯·ä½¿ç”¨ç®€çŸ­è‹±æ–‡åŠ¨è¯æˆ–åŠ¨è¯çŸ­è¯­ï¼ˆå¦‚ IS_A, USES, PART_OF, APPLIED_TO ç­‰ï¼‰
# 5. å¦‚æœå…³ç³»åœ¨è¯­ä¹‰ä¸Šæˆç«‹ï¼Œå³å¯è¾“å‡ºï¼Œä¸å¿…è¿‡åº¦ä¿å®ˆï¼Œä½†ä¹Ÿä¸èƒ½éšä¾¿åˆ›å»ºå…³ç³»
# 6. ä¸è¦è¾“å‡ºä»»ä½•è§£é‡Šæ€§æ–‡å­—

# æ•™ææ–‡æœ¬ï¼š
# {text}
# """

#     kg_extractor = SimpleLLMPathExtractor(
#         llm=llm,
#         extract_prompt=kg_prompt_template,
#         max_paths_per_chunk=15,
#         num_workers=4
#     )

    kg_prompt_template = """
ä½ æ˜¯ä¸€åã€è®¡ç®—æœºç§‘å­¦ã€‘çŸ¥è¯†å›¾è°±æ„å»ºä¸“å®¶ã€‚
è¯·ä»ä¸‹åˆ—æ•™ææ–‡æœ¬ä¸­æå–ã€æ ¸å¿ƒæ¦‚å¿µã€‘åŠå…¶ã€å…³ç³»ã€‘ï¼Œæ„å»ºçŸ¥è¯†ä¸‰å…ƒç»„ã€‚

### ä¸¥æ ¼çº¦æŸï¼š
1. **æ ¼å¼**ï¼šæ¯è¡Œä¸€ä¸ªä¸‰å…ƒç»„ï¼Œä½¿ç”¨ "|" åˆ†éš”ï¼Œæ ¼å¼ä¸ºï¼š`å®ä½“1 | å…³ç³» | å®ä½“2`
2. **æ‹’ç»æ•°å­¦ç¬¦å·**ï¼šä¸è¦æå–çº¯æ•°å­—ï¼ˆå¦‚ "0", "1"ï¼‰ã€å•å­—æ¯å˜é‡ï¼ˆå¦‚ "x", "t"ï¼‰æˆ–å…¬å¼ç‰‡æ®µä½œä¸ºå®ä½“ã€‚
3. **å®ä½“è¦æ±‚**ï¼šå®ä½“å¿…é¡»æ˜¯å…·æœ‰ç‹¬ç«‹è¯­ä¹‰çš„åè¯ï¼ˆå¦‚â€œäº¤å‰ç†µè¯¯å·®â€ã€â€œSoftmaxå‡½æ•°â€ã€â€œç¥ç»ç½‘ç»œâ€ï¼‰ã€‚
4. **å…³ç³»è¦æ±‚**ï¼šå…³ç³»å¿…é¡»æ˜¯åŠ¨è¯æˆ–åŠ¨è¯çŸ­è¯­ï¼ˆå¦‚â€œè®¡ç®—â€ã€â€œå±äºâ€ã€â€œåŒ…å«â€ã€â€œç”¨äºâ€ï¼‰ã€‚
5. **è¯­è¨€**ï¼šä¿æŒå®ä½“ä¸ºä¸­æ–‡ï¼ˆé™¤éåŸæ–‡æ˜¯ä¸“æœ‰åè¯è‹±æ–‡ï¼‰ã€‚

### é”™è¯¯ç¤ºä¾‹ï¼ˆç»å¯¹ä¸è¦è¾“å‡ºï¼‰ï¼š
- 0 | 0 | 1  (ç¦æ­¢çº¯æ•°å­—)
- t | ç­‰äº | (0,0,1) (ç¦æ­¢å…¬å¼)
- è¿™é‡Œçš„ | æ˜¯ | æ ‡ç­¾ (ç¦æ­¢æ— æ„ä¹‰æ–‡æœ¬)

### æ­£ç¡®ç¤ºä¾‹ï¼š
- Softmaxå±‚ | è¾“å‡º | æ¦‚ç‡åˆ†å¸ƒ
- äº¤å‰ç†µè¯¯å·® | ç”¨äºè¡¡é‡ | æŸå¤±
- ç¥ç»ç½‘ç»œ | åŒ…å« | éšè—å±‚

### å¾…å¤„ç†æ–‡æœ¬ï¼š
{text}
"""

    # 3. å®ä¾‹åŒ– Extractor æ—¶ï¼Œä¼ å…¥ parse_fn
    kg_extractor = SimpleLLMPathExtractor(
        llm=llm,
        extract_prompt=kg_prompt_template,
        max_paths_per_chunk=20, # ç¨å¾®è°ƒå¤§ä¸€ç‚¹
        num_workers=4,
        parse_fn=custom_parse_triplets  # <--- å…³é”®ï¼šæ³¨å…¥æˆ‘ä»¬è‡ªå®šä¹‰çš„è§£æå™¨
    )

    index = PropertyGraphIndex.from_documents(  # æŠŠæ–‡æ¡£ï¼Œæå–å™¨ï¼Œå­˜å‚¨å™¨ä¸²è”èµ·æ¥ï¼Œæ‰§è¡Œæµæ°´çº¿å·¥ä½œ
        documents=documents,
        kg_extractors=[kg_extractor],
        embed_model=embed_model, # æ—¢å¯ä»¥åšå›¾æœç´¢ï¼Œä¹Ÿå¯ä»¥å¯¹å›¾ä¸Šçš„èŠ‚ç‚¹åšå‘é‡æœç´¢
        property_graph_store=graph_store,
        show_progress=True,
    )

    print("\nğŸ‰ ================= Success ================= ğŸ‰")
    print("çŸ¥è¯†å›¾è°±æ„å»ºå®Œæˆï¼")
    print(f"æ•°æ®å·²å­˜å…¥ Neo4jã€‚")
    print("ä¸‹ä¸€æ­¥: æ‰“å¼€æµè§ˆå™¨ http://localhost:7474 æŸ¥çœ‹ä½ çš„å›¾è°±ï¼")
    print("æ¨èæŸ¥è¯¢è¯­å¥: MATCH (n)-[r]->(m) RETURN n,r,m LIMIT 50")

if __name__ == "__main__":
    main()