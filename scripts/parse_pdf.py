import os
import sys
import re
import asyncio
import json
from typing import List, Set, Optional, Dict, Any
from pathlib import Path
from dotenv import load_dotenv
import nest_asyncio
import qdrant_client
from qdrant_client.http import models as rest

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° path ä»¥ä¾¿å¯¼å…¥ etl æ¨¡å—
sys.path.append(str(Path(__file__).resolve().parents[1]))

# å¯¼å…¥ä½ çš„ Parser (v1.5.1 / v1.6)
from etl.local_parser import LocalPDFParser

# ç¯å¢ƒé…ç½®
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
load_dotenv()
nest_asyncio.apply()

from llama_index.core.node_parser import HierarchicalNodeParser, get_leaf_nodes
from llama_index.core import Settings, VectorStoreIndex, StorageContext, PropertyGraphIndex
from llama_index.core.graph_stores.types import EntityNode, Relation, KG_NODES_KEY, KG_RELATIONS_KEY
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.graph_stores.neo4j import Neo4jPropertyGraphStore
from llama_index.core.indices.property_graph import SimpleLLMPathExtractor
from llama_index.llms.dashscope import DashScope
from llama_index.vector_stores.qdrant import QdrantVectorStore
from llama_index.core.schema import TransformComponent, BaseNode
from neo4j import GraphDatabase

# --- é…ç½® ---
PDF_PATH = Path(__file__).resolve().parents[1] / "data" / "æ·±åº¦å­¦ä¹ è¿›é˜¶_è‡ªç„¶è¯­è¨€å¤„ç†_æ–‹è—¤åº·æ¯….pdf"

QDRANT_URL = "http://localhost:6333"
CHUNK_COLLECTION = "edu_matrix_chunks"   
ENTITY_COLLECTION = "edu_matrix_entities" 
EMBEDDING_DIM = 1024 

NEO4J_USER = "neo4j"
NEO4J_PASSWORD = os.getenv("NEO4J_PASSWORD") or "password"
NEO4J_URI = "bolt://localhost:7687"

CACHE_DIR = Path(__file__).resolve().parents[1] / "data" / "parser_cache"
SIDECAR_FILE = CACHE_DIR / "page_heavy_data.json"
CACHE_DIR.mkdir(exist_ok=True, parents=True)

# --- æç¤ºè¯ (Prompts) ---
# LlamaIndex ä¼šè‡ªåŠ¨å°† node.text å¡«å…¥ {text} å ä½ç¬¦
KG_EXTRACTION_PROMPT = """
ä½ æ˜¯ä¸€åä¸“é—¨ä»äº‹ã€è®¡ç®—æœºç§‘å­¦ä¸å·¥ç¨‹æ•™è‚²ã€‘çš„çŸ¥è¯†å›¾è°±æ„å»ºä¸“å®¶ã€‚
ä½ çš„ä»»åŠ¡æ˜¯ä»ç»™å®šçš„æ•™æé¡µé¢å†…å®¹ä¸­æå–æ ¸å¿ƒæ¦‚å¿µåŠå…¶é€»è¾‘å…³ç³»ã€‚

### å†…å®¹èƒŒæ™¯ï¼š
è¾“å…¥çš„æ–‡æœ¬ç»è¿‡äº†å¤šæ¨¡æ€é¢„å¤„ç†ï¼ŒåŒ…å«ï¼š
1. [SECTION]: å½“å‰æ‰€å±çš„ç« èŠ‚æ ‡é¢˜ï¼Œæä¾›äº†å®è§‚è¯­å¢ƒã€‚
2. [KEYWORDS]: é¡µé¢é«˜é¢‘æœ¯è¯­ï¼Œæš—ç¤ºäº†æ ¸å¿ƒå®ä½“ã€‚
3. === æ’å›¾æè¿° ===: ç”±è§†è§‰æ¨¡å‹(VLM)ç”Ÿæˆçš„å›¾åƒè¯­ä¹‰ï¼ŒåŒ…å«å›¾ä¸­ç‰¹æœ‰çš„ç»„ä»¶å’Œé€»è¾‘æµã€‚

### æå–è§„åˆ™ï¼š
1. **æ ¼å¼çº¦æŸ**ï¼šæ¯è¡Œä»…è¾“å‡ºä¸€ä¸ªä¸‰å…ƒç»„ï¼Œæ ¼å¼ä¸ºï¼š`å®ä½“1 | å…³ç³» | å®ä½“2`ã€‚
2. **å®ä½“ç²’åº¦**ï¼š
   - ä¼˜å…ˆæå–ä¸“ä¸šæœ¯è¯­ï¼ˆå¦‚â€œåå‘ä¼ æ’­â€ã€â€œæµæ°´çº¿å†’é™©â€ã€â€œè™šæ‹Ÿåœ°å€ç©ºé—´â€ï¼‰ã€‚
   - å¿…é¡»åŒ…å«æ’å›¾æè¿°ä¸­æåˆ°çš„å…³é”®ç»„ä»¶ï¼ˆå¦‚â€œALUâ€ã€â€œå¯„å­˜å™¨æ–‡ä»¶â€ï¼‰ã€‚
   - ç¦æ­¢æå–çº¯æ•°å­—ã€å•å­—æ¯å˜é‡ï¼ˆx, y, iï¼‰æˆ–æ— æ„ä¹‰çš„ä»£è¯ï¼ˆä½œè€…ã€æœ¬æ–‡ã€ä¸‹å›¾ï¼‰ã€‚
3. **å…³ç³»ç±»å‹**ï¼š
   - ä½¿ç”¨å…·ä½“çš„åŠ¨è¯çŸ­è¯­æè¿°é€»è¾‘ï¼š`åŒ…å«`, `å±äº`, `å®ç°`, `è§£å†³`, `å¯¼è‡´`, `æ•°æ®æµå‘`, `æ§åˆ¶ä¿¡å·`, `è®¡ç®—`, `ä¼˜åŒ–`ã€‚
4. **è¯­è¨€è¦æ±‚**ï¼šä¿æŒå®ä½“åç§°ä¸åŸæ–‡ä¸€è‡´ï¼ˆä¸­è‹±æ–‡æ··æ’ï¼‰ã€‚
5. **æ‹’ç»å¹»è§‰**ï¼šä»…æ ¹æ®æä¾›çš„æ–‡æœ¬æå–ï¼Œä¸è¦å¼•å…¥å¤–éƒ¨çŸ¥è¯†æˆ–è¾“å‡ºè§£é‡Šæ€§æ–‡å­—ã€‚

### æ­£ç¡®ç¤ºä¾‹ï¼š
- æ¢¯åº¦æ¶ˆå¤± | å¯¼è‡´ | æƒé‡æ›´æ–°ç¼“æ…¢
- ReLUå‡½æ•° | è§£å†³ | æ¢¯åº¦æ¶ˆå¤±é—®é¢˜
- æ’å›¾1 | å±•ç¤ºäº† | MIPSäº”çº§æµæ°´çº¿ç»“æ„
- è¯‘ç é˜¶æ®µ | ç”Ÿæˆ | æ§åˆ¶ä¿¡å·

### å¾…å¤„ç†æ–‡æœ¬ï¼š
{text}
"""

# ==========================================
# ğŸ› ï¸ å·¥å…·å‡½æ•°ï¼šQdrant é›†åˆæ£€æŸ¥ä¸åˆ›å»º (å«ç´¢å¼•ä¼˜åŒ–)
# ==========================================
def check_and_create_collection(
    client: qdrant_client.QdrantClient, 
    collection_name: str, 
    vector_size: int,
    hnsw_config: Optional[rest.HnswConfigDiff] = None,
    recreate: bool = False
):
    """
    æ£€æŸ¥é›†åˆã€‚å¦‚æœ recreate=Trueï¼Œåˆ™å…ˆåˆ é™¤æ—§é›†åˆå†åˆ›å»ºï¼Œé˜²æ­¢æ•°æ®é‡å¤å åŠ ã€‚
    """
    if recreate and client.collection_exists(collection_name):
        print(f"â™»ï¸  æ­£åœ¨åˆ é™¤æ—§é›†åˆ {collection_name} (Recreate Mode)...")
        client.delete_collection(collection_name)

    if not client.collection_exists(collection_name):
        print(f"âš ï¸ é›†åˆ {collection_name} ä¸å­˜åœ¨ï¼Œæ­£åœ¨åˆ›å»º (Dim: {vector_size})...")
        client.create_collection(
            collection_name=collection_name,
            vectors_config=rest.VectorParams(
                size=vector_size,
                distance=rest.Distance.COSINE,
                on_disk=True, # [ä¼˜åŒ–] å¼€å¯ç£ç›˜å­˜å‚¨
            ),
            hnsw_config=hnsw_config 
        )
        
        # [ä¼˜åŒ–] åˆ›å»º Payload ç´¢å¼•åŠ é€Ÿè¿‡æ»¤
        client.create_payload_index(collection_name, "page", rest.PayloadSchemaType.INTEGER)
        client.create_payload_index(collection_name, "chunk_type", rest.PayloadSchemaType.KEYWORD)
        client.create_payload_index(collection_name, "section_title", rest.PayloadSchemaType.TEXT)
        
        print(f"âœ… é›†åˆ {collection_name} åˆ›å»ºå¹¶åˆå§‹åŒ–å®Œæˆ")
    else:
        print(f"âœ… é›†åˆ {collection_name} å·²å­˜åœ¨ (è·³è¿‡åˆ›å»º)")

# ==========================================
# ğŸ§  Sidecar Aware Extractor (ä¿®å¤ Pydantic Init æŠ¥é”™)
# ==========================================
class MetadataGraphExtractor(TransformComponent):
    # æ˜¾å¼å£°æ˜å­—æ®µç±»å‹
    sidecar_data: Dict[str, Any] = {}

    def __init__(self, sidecar_path: Path, **kwargs):
        # 1. å…ˆè°ƒç”¨çˆ¶ç±»åˆå§‹åŒ– (ä¸å¸¦ sidecar_dataï¼Œé˜²æ­¢ Pylance æŠ¥é”™)
        super().__init__(**kwargs)
        
        # 2. æ‰‹åŠ¨åŠ è½½æ•°æ®å¹¶èµ‹å€¼ç»™å­—æ®µ
        data = {}
        if sidecar_path.exists():
            try:
                with open(sidecar_path, "r", encoding="utf-8") as f:
                    data = json.load(f)
            except Exception as e:
                print(f"âš ï¸ è¯»å– Sidecar æ–‡ä»¶å¤±è´¥: {e}")
        
        # ç›´æ¥èµ‹å€¼ï¼ŒPydantic ä¼šå¤„ç†
        self.sidecar_data = data

    def __call__(self, nodes: List[BaseNode], **kwargs) -> List[BaseNode]:
        for node in nodes:
            page_id_key = node.metadata.get("page_id_key")
            if not page_id_key: continue

            # ä» sidecar è·å–å›¾è°±æ•°æ®
            heavy_data = self.sidecar_data.get(page_id_key, {})
            graph_data = heavy_data.get("graph_data", {})
            
            if not graph_data: continue

            # [ä¼˜åŒ–] ä½¿ç”¨ list() å¤åˆ¶é˜²æ­¢å¼•ç”¨æ±¡æŸ“ï¼Œå¢åŠ  set() å»é‡é˜²æ­¢é‡å¤æ·»åŠ 
            existing_nodes = list(node.metadata.get(KG_NODES_KEY, []))
            existing_relations = list(node.metadata.get(KG_RELATIONS_KEY, []))
            
            seen_nodes: Set[str] = set(n.name for n in existing_nodes)
            seen_rels: Set[str] = set(f"{r.source_id}-{r.target_id}-{r.label}" for r in existing_relations)

            # æ³¨å…¥ VLM è¯†åˆ«çš„å®ä½“
            for entity in graph_data.get("entities", []):
                name = entity.get("name")
                label = entity.get("category", "Concept")
                if name and name not in seen_nodes:
                    existing_nodes.append(EntityNode(name=name, label=label, properties=entity))
                    seen_nodes.add(name)

            # æ³¨å…¥ VLM è¯†åˆ«çš„å…³ç³»
            for rel in graph_data.get("relations", []):
                src = rel.get("source")
                tgt = rel.get("target")
                label = rel.get("relation", "RELATED_TO")
                rel_key = f"{src}-{tgt}-{label}"
                
                if src and tgt and rel_key not in seen_rels:
                    existing_relations.append(Relation(source_id=src, target_id=tgt, label=label, properties=rel))
                    seen_rels.add(rel_key)

            node.metadata[KG_NODES_KEY] = existing_nodes
            node.metadata[KG_RELATIONS_KEY] = existing_relations
            
        return nodes

def custom_parse_triplets(llm_output: str):
    """æ¸…æ´— LLM æå–çš„ä¸‰å…ƒç»„"""
    triplets = []
    lines = llm_output.strip().split("\n")
    for line in lines:
        if len(line) < 5: continue
        parts = [p.strip() for p in line.split("|") if p.strip()]
        if len(parts) == 3:
            subj, pred, obj = parts[0].strip(), parts[1].strip(), parts[2].strip()
            if len(subj) < 2 or len(obj) < 2: continue
            if re.match(r'^[\d\(\)\[\],.=\s%<>\-\+\*\/\\a-zA-Z0-9]+$', subj) and len(subj) < 4: continue
            if re.match(r'^[\d\(\)\[\],.=\s%<>\-\+\*\/\\a-zA-Z0-9]+$', obj) and len(obj) < 4: continue
            if "here are" in subj.lower() or "example" in subj.lower(): continue
            triplets.append((subj, pred, obj))
    return triplets

async def main():
    print(f"ğŸš€ [Async] å¼€å§‹ Pipeline: {PDF_PATH}")
    
    # 1. è§£æ (ä½¿ç”¨ v1.5+ Sidecar æ¨¡å¼ Parser)
    parser = LocalPDFParser(
        pdf_path=PDF_PATH,
        image_output_dir=str(CACHE_DIR / "images"),
        cache_file=str(CACHE_DIR / "vlm_cache.json"),
        hash_record_file=str(CACHE_DIR / "processed_hashes.json"),
        sidecar_file=str(SIDECAR_FILE),
        use_vlm=True,
        max_concurrency=5
    )
    documents = await parser.parse()
    # documents = documents[10:15] # è°ƒè¯•åˆ‡ç‰‡
    print(f"âœ… è§£æå®Œæˆï¼Œè·å¾— {len(documents)} ä¸ªé¡µé¢çº§æ–‡æ¡£")

    # 2. Parent-Child åˆ‡åˆ†
    # [ä¼˜åŒ–] Chunk Size: [800, 200] é€‚é…æ•™æå¯†åº¦
    node_parser = HierarchicalNodeParser.from_defaults(chunk_sizes=[800, 200])
    nodes = node_parser.get_nodes_from_documents(documents)
    
    leaf_nodes = get_leaf_nodes(nodes)
    parent_nodes = [n for n in nodes if n.node_id not in set(x.node_id for x in leaf_nodes)]

    for n in parent_nodes:
        n.metadata["chunk_type"] = "parent"
        if n.metadata.get("page_label"): n.metadata["page"] = int(n.metadata["page_label"])

    for n in leaf_nodes:
        n.metadata["chunk_type"] = "leaf"
        if n.metadata.get("page_label"): n.metadata["page"] = int(n.metadata["page_label"])
    
    print(f"ğŸ“Š åˆ‡åˆ†ç»Ÿè®¡: çˆ¶èŠ‚ç‚¹ {len(parent_nodes)} ä¸ª, å¶å­èŠ‚ç‚¹ {len(leaf_nodes)} ä¸ª")

    # 3. åˆå§‹åŒ– & é›†åˆç®¡ç†
    client = qdrant_client.QdrantClient(url=QDRANT_URL)
    
    # [å…³é”®] Recreate=True é˜²æ­¢é‡å¤æ•°æ®å †å  (ç¬¬ä¸€æ¬¡è·‘æˆ–å…¨é‡æ›´æ–°æ—¶å¼€å¯)
    FORCE_RECREATE = True 
    
    check_and_create_collection(client, CHUNK_COLLECTION, EMBEDDING_DIM, recreate=FORCE_RECREATE)
    
    # [ä¼˜åŒ–] å®ä½“é›†åˆä½¿ç”¨å†…å­˜ä¼˜åŒ–ç‰ˆ HNSW
    entity_hnsw = rest.HnswConfigDiff(m=16, ef_construct=64)
    check_and_create_collection(client, ENTITY_COLLECTION, EMBEDDING_DIM, hnsw_config=entity_hnsw, recreate=FORCE_RECREATE)
    
    embed_model = HuggingFaceEmbedding(model_name="BAAI/bge-m3", trust_remote_code=True)
    Settings.embed_model = embed_model
    llm = DashScope(model_name=os.getenv("DASHSCOPE_MODEL_NAME", "qwen-plus"), api_key=os.getenv("DASHSCOPE_API_KEY"), temperature=0.1)
    Settings.llm = llm

    # 4. æ„å»º Chunk å‘é‡ç´¢å¼•
    print(f"\nğŸ§  [Step 1/2] æ„å»º Chunk ç´¢å¼• ({CHUNK_COLLECTION})...")
    vector_store_chunks = QdrantVectorStore(client=client, collection_name=CHUNK_COLLECTION)
    storage_context_chunks = StorageContext.from_defaults(vector_store=vector_store_chunks)
    
    # åªä¸º Leaf Nodes å»ºç«‹å‘é‡ç´¢å¼•
    VectorStoreIndex(
        leaf_nodes,
        storage_context=storage_context_chunks,
        show_progress=True,
    )

    # 5. æ„å»º Graph
    print(f"\nğŸ•¸ï¸ [Step 2/2] æ„å»ºçŸ¥è¯†å›¾è°± ({ENTITY_COLLECTION})...")
    
    # å¦‚æœå¼ºåˆ¶é‡å»ºï¼Œé¡ºä¾¿æ¸…ç©º Neo4j (å¼€å‘æ¨¡å¼å®‰å…¨æªæ–½)
    graph_store = Neo4jPropertyGraphStore(username=NEO4J_USER, password=NEO4J_PASSWORD, url=NEO4J_URI)
    if FORCE_RECREATE:
        print("âš ï¸ [DEV] æ­£åœ¨æ¸…ç©º Neo4j æ•°æ®åº“...")
        try:
            with GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD)) as driver:
                driver.execute_query("MATCH (n) DETACH DELETE n")
            print("âœ… Neo4j æ¸…ç©ºå®Œæˆ")
        except Exception as e:
            print(f"Neo4j æ¸…ç©ºå¤±è´¥ (å¯èƒ½ä¸ºç©º): {e}")

    vector_store_entities = QdrantVectorStore(client=client, collection_name=ENTITY_COLLECTION)

    # æŠ½å–å™¨é…ç½®
    llm_extractor = SimpleLLMPathExtractor(
        llm=llm,
        extract_prompt=KG_EXTRACTION_PROMPT, # ä½¿ç”¨æ–°çš„è¯¦ç»†æç¤ºè¯ (LlamaIndex ä¼šè‡ªåŠ¨å¡«å…… {text})
        max_paths_per_chunk=10, # [æˆæœ¬æ§åˆ¶] é€‚åº¦é™ä½æ¯å—æå–æ•°é‡
        num_workers=4,
        parse_fn=custom_parse_triplets
    )
    
    # ä¼ å…¥ Sidecar è·¯å¾„åˆå§‹åŒ– MetadataGraphExtractor
    metadata_extractor = MetadataGraphExtractor(sidecar_path=SIDECAR_FILE)

    PropertyGraphIndex(
        nodes=parent_nodes,
        kg_extractors=[metadata_extractor, llm_extractor],
        llm=llm,
        embed_model=embed_model,
        property_graph_store=graph_store,
        vector_store=vector_store_entities,
        # [ä¼˜åŒ–] æ˜¾å¼å¼€å¯ KG Embedding
        embed_kg_nodes=True, 
        show_progress=True,
    )

    print("\nğŸ‰ ================= Pipeline Completed ================= ğŸ‰")
    print(f"âœ… Chunk Size: 800/200 optimized")
    print(f"âœ… Entity HNSW: Memory optimized")
    print(f"âœ… Graph Embedding: Enabled")
    print(f"âœ… Dual-Source Fusion: Active")

if __name__ == "__main__":
    asyncio.run(main())