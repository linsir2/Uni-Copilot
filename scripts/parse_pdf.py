import os
import sys
import re
import asyncio
import json
import argparse
from typing import List, Optional, Dict, Any, cast
from pathlib import Path
from dotenv import load_dotenv
import nest_asyncio
import qdrant_client
from qdrant_client.http import models as rest
from pydantic import PrivateAttr 

# Add project root to path for importing etl module
sys.path.append(str(Path(__file__).resolve().parents[1]))

# Import your Final Production Parser & Upserter
from etl.local_parser import LocalPDFParser
from etl.entity_upsert import upsert_entities

# Environment Config
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
load_dotenv()
nest_asyncio.apply()

from llama_index.core.node_parser import HierarchicalNodeParser, get_leaf_nodes
from llama_index.core import Settings, VectorStoreIndex, StorageContext, PropertyGraphIndex
from llama_index.core.graph_stores.types import EntityNode, Relation, KG_NODES_KEY, KG_RELATIONS_KEY
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.graph_stores.neo4j import Neo4jPropertyGraphStore
from llama_index.core.indices.property_graph import SimpleLLMPathExtractor 
from llama_index.llms.dashscope import DashScope
from llama_index.vector_stores.qdrant import QdrantVectorStore
from llama_index.core.schema import TransformComponent, BaseNode
from neo4j import GraphDatabase

# --- Configuration ---
QDRANT_URL = "http://localhost:6333"
CHUNK_COLLECTION = "edu_matrix_chunks"    
ENTITY_COLLECTION = "edu_matrix_entities" 
EMBEDDING_DIM = 1024 

NEO4J_USER = "neo4j"
NEO4J_PASSWORD = os.getenv("NEO4J_PASSWORD") or "password"
NEO4J_URI = "bolt://localhost:7687"

# --- Embedding Wrapper ---
def embed_text_wrapper(text: str) -> List[float]:
    """Safe wrapper for LlamaIndex embedding models"""
    
    if hasattr(Settings.embed_model, "get_text_embedding"):
        return Settings.embed_model.get_text_embedding(text)
    elif hasattr(Settings.embed_model, "get_query_embedding"):
        return Settings.embed_model.get_query_embedding(text)
    else:
        dynamic_model = cast(Any, Settings.embed_model)
        if hasattr(dynamic_model, "encode"):
            result = dynamic_model.encode(text)
            return result.tolist() if hasattr(result, "tolist") else result
            
    raise RuntimeError("Embedding model interface not supported")

# --- Prompts ---
KG_EXTRACTION_PROMPT = """
You are a Knowledge Graph expert specialized in **Computer Science & Engineering Education**.
Your task is to extract core concepts and logical relationships from the provided textbook page content.

### Content Context:
The input text has undergone multimodal preprocessing, including:
1. [SECTION]: Current section title, providing macro context.
2. [KEYWORDS]: Frequent terms on the page, hinting at core entities.
3. === Figure Description ===: Semantic description generated by a Visual Language Model (VLM).

### Extraction Rules:
1. **Format Constraint**: Output one triplet per line in the format: `Entity1 | Relation | Entity2`.

2. **Entity Granularity & Cleaning**:
   - Prioritize technical terms (e.g., "Backpropagation", "Pipeline Hazard").
   - MUST include key components mentioned in Figure Descriptions (e.g., "ALU").
   - **STRICT PROHIBITION on Pronouns**: Never extract generic pronouns or demonstratives as entities. 
     - Forbidden English: "it", "this", "that", "the model", "the method", "the algorithm".
     - Forbidden Chinese: "ÂÆÉ", "Ëøô‰∏™", "ËØ•Ê®°Âûã", "Ê≠§ÁÆóÊ≥ï", "Êú¨ËäÇ", "ËØ•ÊñπÊ≥ï".
   - **Coreference Resolution**: If the source text uses a pronoun to refer to a specific concept, you MUST replace it with the full, explicit name of that concept (e.g., replace "it" or "ÂÆÉ" with "Convolutional Neural Network").
   - DO NOT extract pure numbers, single-letter variables, or generic textbook terms (e.g., "Example", "Exercise").

3. **Relation Types**:
   - Use specific verb phrases: `CONTAINS`, `BELONGS_TO`, `IMPLEMENTS`, `SOLVES`, `CAUSES`, `DATA_FLOWS_TO`, `CONTROLS`, `CALCULATES`, `OPTIMIZES`.

4. **No Hallucination**: Extract strictly based on the provided text. Maintain the original language of the technical terms if they are in Chinese.

### Input Text:
{text}
"""

# ==========================================
# üõ†Ô∏è Utility: Qdrant Collection Management
# ==========================================
def check_and_create_collection(
    client: qdrant_client.QdrantClient, 
    collection_name: str, 
    vector_size: int,
    hnsw_config: Optional[rest.HnswConfigDiff] = None,
    recreate: bool = False,
    is_entity_collection: bool = False
):
    if recreate and client.collection_exists(collection_name):
        print(f"‚ôªÔ∏è  Deleting old collection {collection_name} (Recreate Mode)...")
        client.delete_collection(collection_name)

    if not client.collection_exists(collection_name):
        print(f"‚ö†Ô∏è Collection {collection_name} does not exist, creating (Dim: {vector_size})...")
        client.create_collection(
            collection_name=collection_name,
            vectors_config=rest.VectorParams(
                size=vector_size,
                distance=rest.Distance.COSINE,
                on_disk=True, 
            ),
            hnsw_config=hnsw_config 
        )
        
        client.create_payload_index(collection_name, "page", rest.PayloadSchemaType.INTEGER)
        
        if is_entity_collection:
            client.create_payload_index(collection_name, "name", rest.PayloadSchemaType.KEYWORD)
            print(f"   --> Indexing 'name' field for entities.")
            client.create_payload_index(collection_name, "normalized_name", rest.PayloadSchemaType.KEYWORD)
        else:
            client.create_payload_index(collection_name, "chunk_type", rest.PayloadSchemaType.KEYWORD)
            client.create_payload_index(collection_name, "section_title", rest.PayloadSchemaType.TEXT)
            print(f"   --> Indexing 'chunk_type' & 'section_title' for chunks.")
            
        print(f"‚úÖ Collection {collection_name} initialized.")
    else:
        print(f"‚úÖ Collection {collection_name} exists.")

# ==========================================
# üß† Sidecar Aware Extractor
# ==========================================
class MetadataGraphExtractor(TransformComponent):
    _sidecar_data: Dict[str, Any] = PrivateAttr(default_factory=dict)

    def __init__(self, sidecar_path: Path, **kwargs):
        super().__init__(**kwargs)
        if sidecar_path.exists():
            try:
                with open(sidecar_path, "r", encoding="utf-8") as f:
                    self._sidecar_data = json.load(f)
                print(f"üìÇ Loaded Sidecar Data: {len(self._sidecar_data)} pages found.")
            except Exception as e:
                print(f"‚ö†Ô∏è Failed to read Sidecar file: {e}")
        else:
            print(f"‚ö†Ô∏è Sidecar file not found: {sidecar_path}")

    def __call__(self, nodes: List[BaseNode], **kwargs) -> List[BaseNode]:
        for node in nodes:
            page_id_key = node.metadata.get("page_id_key")
            if not page_id_key: continue

            heavy_data = self._sidecar_data.get(page_id_key, {})
            graph_data = heavy_data.get("graph_data", {})
            if not graph_data: continue

            existing_nodes = node.metadata.get(KG_NODES_KEY, [])
            existing_relations = node.metadata.get(KG_RELATIONS_KEY, [])
            
            seen_nodes = set(n.name for n in existing_nodes)
            seen_rels = set(f"{r.source_id}-{r.target_id}-{r.label}" for r in existing_relations)

            for entity in graph_data.get("entities", []):
                name = entity.get("name")
                label = entity.get("category", "Concept")
                if name and name not in seen_nodes:
                    existing_nodes.append(EntityNode(name=name, label=label, properties=entity))
                    seen_nodes.add(name)

            for rel in graph_data.get("relations", []):
                src = rel.get("source")
                tgt = rel.get("target")
                label = rel.get("relation", "RELATED_TO")
                rel_key = f"{src}-{tgt}-{label}"
                
                if src and tgt and rel_key not in seen_rels:
                    existing_relations.append(Relation(source_id=src, target_id=tgt, label=label, properties=rel))
                    seen_rels.add(rel_key)

            node.metadata[KG_NODES_KEY] = existing_nodes
            node.metadata[KG_RELATIONS_KEY] = existing_relations
            
        return nodes

def custom_parse_triplets(llm_output: str):
    STOP_CONCEPTS = {
        "it", "this", "that", "they", "these", "those", "we", "us", 
        "figure", "table", "section", "chapter", "the model", "the method", 
        "this paper", "the algorithm", "example", "image", "diagram", "below", "above",
        "ÂÆÉ", "‰ªñ‰ª¨", "Ëøô‰∏™", "ÈÇ£‰∏™", "Ëøô‰∫õ", "ÈÇ£‰∫õ", "ËØ•Ê®°Âûã", "Ê≠§ÁÆóÊ≥ï", 
        "Êú¨ËäÇ", "‰∏äÂõæ", "‰∏ãÂõæ", "Ë°®", "Âõæ", "‰æãÂ≠ê", "Â¶Ç‰∏ã", "ËøôÁßçÊñπÊ≥ï",
    }
    triplets = []
    lines = llm_output.strip().split("\n")
    for line in lines:
        if len(line) < 5: continue
        parts = [p.strip() for p in line.split("|") if p.strip()]
        if len(parts) == 3:
            subj, pred, obj = parts[0].strip(), parts[1].strip(), parts[2].strip()
            
            if len(subj) < 2 or len(obj) < 2: continue

            s_low, o_low = subj.lower(), obj.lower()
            if s_low in STOP_CONCEPTS or o_low in STOP_CONCEPTS:
                continue
            if re.match(r'^[\d\(\)\[\],.=\s%<>\-\+\*\/\\a-zA-Z0-9]+$', subj) and len(subj) < 4: continue
            
            triplets.append((subj, pred, obj))
    return triplets

async def main(pdf_path_str: str, force_recreate: bool = True):
    pdf_path = Path(pdf_path_str)
    if not pdf_path.exists():
        print(f"‚ùå File not found: {pdf_path}")
        return 
    
    print(f"üöÄ [Async] Starting Pipeline: {pdf_path.name}")

    safe_name = pdf_path.stem.replace(" ", "_")
    base_data_dir = Path(__file__).resolve().parents[1] / "data"

    DYNAMIC_CACHE_DIR = base_data_dir / "parser_cache" / safe_name
    DYNAMIC_SIDECAR_FILE = DYNAMIC_CACHE_DIR / "page_heavy_data.json"
    DYNAMIC_CACHE_DIR.mkdir(parents=True, exist_ok=True)
    
    # 1. Init Database Connections & Clear (If Force Recreate)
    client = qdrant_client.QdrantClient(url=QDRANT_URL)
    
    if force_recreate:
        print("\nüßπ [Clean Start] Recreating collections & Clearing Neo4j...")
        # Clear Qdrant
        check_and_create_collection(client, CHUNK_COLLECTION, EMBEDDING_DIM, recreate=True, is_entity_collection=False)
        entity_hnsw = rest.HnswConfigDiff(m=16, ef_construct=64)
        check_and_create_collection(client, ENTITY_COLLECTION, EMBEDDING_DIM, hnsw_config=entity_hnsw, recreate=True, is_entity_collection=True)
        
        # Clear Neo4j & Create Index
        try:
            with GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD)) as driver:
                with driver.session() as session:
                    # Clear Data
                    session.run("MATCH (n) DETACH DELETE n")
                    print("   -> Data cleared.")
                    
                    # [Optimization] Create Indices for Speed (Crucial for Step 4 & 7)
                    index_queries = [
                        "CREATE INDEX entity_name_index IF NOT EXISTS FOR (n:Entity) ON (n.name)",
                        "CREATE INDEX concept_name_index IF NOT EXISTS FOR (n:Concept) ON (n.name)"
                    ]
                    for idx_q in index_queries:
                        try:
                            session.run(cast(Any, idx_q))
                        except Exception as index_err:
                            print(f"   ‚ö†Ô∏è Index creation skipped (non-critical): {index_err}")
                    
            print("‚úÖ Neo4j cleared and optimized.")
        except Exception as e:
            print(f"Neo4j setup failed: {e}")
    else:
        print("\n‚è≠Ô∏è [Incremental Mode] Skipping collection/DB clearing.")
        check_and_create_collection(client, CHUNK_COLLECTION, EMBEDDING_DIM, recreate=False)
        entity_hnsw = rest.HnswConfigDiff(m=16, ef_construct=64)
        check_and_create_collection(client, ENTITY_COLLECTION, EMBEDDING_DIM, hnsw_config=entity_hnsw, recreate=False, is_entity_collection=True)

    # 2. Parse & Extract Sidecar
    parser = LocalPDFParser(
        pdf_path=pdf_path,
        image_output_dir=str(DYNAMIC_CACHE_DIR / "images"),
        cache_file=str(DYNAMIC_CACHE_DIR / "vlm_cache.json"),
        hash_record_file=str(DYNAMIC_CACHE_DIR / "processed_hashes.json"),
        sidecar_file=str(DYNAMIC_SIDECAR_FILE),
        embedding_cache_file=str(DYNAMIC_CACHE_DIR / "embedding_cache.json"),
        alias_map_file=str(DYNAMIC_CACHE_DIR / "global_alias_map.json"),
        use_vlm=True,
        max_concurrency=5
    )
    documents = await parser.parse()
    print(f"‚úÖ Parsing complete. Acquired {len(documents)} page-level documents.")

    # 3. Init Embeddings & LLM
    embed_model = HuggingFaceEmbedding(model_name="BAAI/bge-m3", trust_remote_code=True)
    Settings.embed_model = embed_model
    llm = DashScope(model_name=os.getenv("DASHSCOPE_MODEL_NAME", "qwen-plus"), api_key=os.getenv("DASHSCOPE_API_KEY"), temperature=0.1)
    Settings.llm = llm

    # 4. [Upsert] Batch Upsert Entities (Qdrant + Neo4j Base Nodes)
    # This now happens SAFELY because Neo4j was cleared in Step 1
    if DYNAMIC_SIDECAR_FILE.exists():
        print("\nüîÑ [Upsert] Processing Entities from Sidecar...")
        try:
            with open(DYNAMIC_SIDECAR_FILE, "r", encoding="utf-8") as f:
                sidecar_data = json.load(f)
            
            entities_map = {}
            for pid, pdata in sidecar_data.items():
                for e in pdata.get("graph_data", {}).get("entities", []):
                    name = e.get("name")
                    if not name: continue
                    props = {k: v for k, v in e.items() if k not in ["name", "canonical_name", "original_name"]}
                    if name not in entities_map:
                        entities_map[name] = {"name": name, "properties": props}
            
            unique_entities = list(entities_map.values())
            print(f"   -> Found {len(unique_entities)} unique entities in Sidecar.")

            metrics = upsert_entities(
                qdrant_client=client,
                qdrant_collection=ENTITY_COLLECTION,
                neo4j_uri=NEO4J_URI,
                neo4j_user=NEO4J_USER,
                neo4j_password=NEO4J_PASSWORD,
                entities=unique_entities,
                embed_fn=embed_text_wrapper 
            )
            print(f"   -> Upsert Metrics: {metrics}")

        except Exception as e:
            print(f"‚ö†Ô∏è Entity Upsert Failed: {e}")

    # 5. Chunk Splitting
    node_parser = HierarchicalNodeParser.from_defaults(chunk_sizes=[800, 200])
    nodes = node_parser.get_nodes_from_documents(documents)
    
    leaf_nodes = get_leaf_nodes(nodes)
    parent_nodes = [n for n in nodes if n.node_id not in set(x.node_id for x in leaf_nodes)]

    for n in parent_nodes:
        n.metadata["chunk_type"] = "parent"
        if n.metadata.get("page_label"): n.metadata["page"] = int(n.metadata["page_label"])
    for n in leaf_nodes:
        n.metadata["chunk_type"] = "leaf"
        if n.metadata.get("page_label"): n.metadata["page"] = int(n.metadata["page_label"])
    
    # 6. Build Chunk Index (Vector Store)
    if force_recreate:
        print(f"\nüß† [Step 1/2] Building Chunk Index ({CHUNK_COLLECTION})...")
        vector_store_chunks = QdrantVectorStore(client=client, collection_name=CHUNK_COLLECTION)
        storage_context_chunks = StorageContext.from_defaults(vector_store=vector_store_chunks)
        
        all_chunks = leaf_nodes + parent_nodes
        VectorStoreIndex(
            all_chunks,
            storage_context=storage_context_chunks,
            show_progress=True,
        )
    else:
        print(f"\n‚è© [Skip] Chunk Index build skipped (Incremental mode).")

    # 7. Build Graph (Relations & Extensions)
    print(f"\nüï∏Ô∏è [Step 2/2] Building Knowledge Graph ({ENTITY_COLLECTION})...")
    
    # [FIXED] Removed the duplicate "Clear Neo4j" block from here!
    
    graph_store = Neo4jPropertyGraphStore(username=NEO4J_USER, password=NEO4J_PASSWORD, url=NEO4J_URI)
    vector_store_entities = QdrantVectorStore(client=client, collection_name=ENTITY_COLLECTION)

    llm_extractor = SimpleLLMPathExtractor(
        llm=llm,
        extract_prompt=KG_EXTRACTION_PROMPT,
        max_paths_per_chunk=10,
        num_workers=4, 
        parse_fn=custom_parse_triplets 
    )
    metadata_extractor = MetadataGraphExtractor(sidecar_path=DYNAMIC_SIDECAR_FILE)

    PropertyGraphIndex(
        nodes=parent_nodes, 
        kg_extractors=[metadata_extractor, llm_extractor],
        llm=llm,
        embed_model=embed_model,
        property_graph_store=graph_store,
        vector_store=vector_store_entities,
        # Ensure we don't re-embed what Upsert already handled
        embed_kg_nodes=False, 
        show_progress=True,
    )

    print("\nüéâ ================= Pipeline Completed ================= üéâ")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Uni-Copilot Ingestion Pipeline")
    parser.add_argument("--file", type=str, required=True, help="Path to PDF")
    parser.add_argument("--recreate", action="store_true", help="Force recreate collections")
    args = parser.parse_args()
    
    asyncio.run(main(pdf_path_str=args.file, force_recreate=args.recreate))