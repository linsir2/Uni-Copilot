import os
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'

from contextlib import asynccontextmanager
from dotenv import load_dotenv
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from fastapi.responses import StreamingResponse

# LlamaIndex æ ¸å¿ƒç»„ä»¶
from llama_index.core import Settings, VectorStoreIndex, PropertyGraphIndex
from llama_index.llms.dashscope import DashScope
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.core.chat_engine import ContextChatEngine
from llama_index.core.memory import ChatMemoryBuffer
from llama_index.core.llms import ChatMessage
from llama_index.core.retrievers import BaseRetriever

# æ•°æ®åº“ç»„ä»¶
from llama_index.vector_stores.qdrant import QdrantVectorStore
from llama_index.graph_stores.neo4j import Neo4jPropertyGraphStore
import qdrant_client

# 1. åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# é…ç½®å‚æ•°
QDRANT_URL = "http://localhost:6333"
QDRANT_COLLECTION = "edu_matrix_v2"
NEO4J_URL = "bolt://localhost:7687"
NEO4J_USER = "neo4j"
NEO4J_PASSWORD = "password123"

# å…¨å±€å¼•æ“å®¹å™¨
rag_engine = {}

# ==========================================
# ğŸ”§ å·¥å…·ç±»å®šä¹‰
# ==========================================

class HybridRetriever(BaseRetriever):
    """
    æ··åˆæ£€ç´¢å™¨ï¼šåŒæ—¶ä» Vector (Qdrant) å’Œ Graph (Neo4j) æ£€ç´¢ï¼Œå¹¶åˆå¹¶ç»“æœã€‚
    """
    def __init__(self, vector_retriever, graph_retriever):
        self.vector_retriever = vector_retriever
        self.graph_retriever = graph_retriever
        super().__init__()
    
    def _retrieve(self, query_bundle):
        # 1. å¹¶è¡Œæ£€ç´¢
        nodes_vect = self.vector_retriever.retrieve(query_bundle)
        nodes_graph = self.graph_retriever.retrieve(query_bundle)
        
        # 2. åˆå¹¶å»é‡ (åŸºäº node_id)
        combined_dict = {n.node.node_id: n for n in (nodes_vect + nodes_graph)}
        return list(combined_dict.values())

# ==========================================
# ğŸš€ ç”Ÿå‘½å‘¨æœŸç®¡ç† (åˆå§‹åŒ–æ ¸å¿ƒ)
# ==========================================

@asynccontextmanager
async def lifespan(app: FastAPI):
    print("ğŸš€ [Startup] æ­£åœ¨åˆå§‹åŒ– EduMatrix å¼•æ“...")
    
    try:
        # 1. åˆå§‹åŒ–æ¨¡å‹ (Embedding + LLM)
        print("ğŸ§  åŠ è½½æ¨¡å‹ (Embedding: BGE-M3, LLM: Qwen)...")
        embed_model = HuggingFaceEmbedding(
            model_name="BAAI/bge-m3",
            trust_remote_code=True,
            local_files_only=True,
            device="cpu",
        )
        Settings.embed_model = embed_model

        llm = DashScope(
            model_name=os.getenv("DASHSCOPE_MODEL_NAME"),
            api_key=os.getenv("DASHSCOPE_API_KEY"),
            temperature=0.1,
        )

        # 2. è¿æ¥ Qdrant (è´Ÿè´£åŸæ–‡æ£€ç´¢)
        print("ğŸ”Œ è¿æ¥ Qdrant (Vector Store)...")
        qdrant_client_obj = qdrant_client.QdrantClient(url=QDRANT_URL)
        vector_store = QdrantVectorStore(
            collection_name=QDRANT_COLLECTION,
            client=qdrant_client_obj,
        )
        vector_index = VectorStoreIndex.from_vector_store(vector_store=vector_store)

        # 3. è¿æ¥ Neo4j (è´Ÿè´£å…³ç³»æ£€ç´¢)
        print("ğŸ”Œ è¿æ¥ Neo4j (Graph Store)...")
        graph_store = Neo4jPropertyGraphStore(
            username=NEO4J_USER,
            password=NEO4J_PASSWORD,
            url=NEO4J_URL,
        )
        graph_index = PropertyGraphIndex.from_existing(
            property_graph_store=graph_store,
            llm=llm
        )

        # 4. æ„å»ºæ··åˆæ£€ç´¢ç­–ç•¥
        # A. å‘é‡æ£€ç´¢å·¥å…·
        vector_tool = vector_index.as_retriever(similarity_top_k=5)
        
        # B. å›¾è°±æ£€ç´¢å·¥å…· (ä½¿ç”¨ VectorContextRetriever è¿›è¡Œå®šä½ + æ‰©æ•£)
        from llama_index.core.retrievers import VectorContextRetriever
        sub_retriever = VectorContextRetriever(
            graph_store=graph_store,
            similarity_top_k=5,
            path_depth=2 # æŠ“å– 2 è·³é‚»å±…
        )
        graph_tool = graph_index.as_retriever(
            sub_retrievers=[sub_retriever]
        )
        
        # C. ç»„è£…æ··åˆæ£€ç´¢å™¨
        hybrid_retriever = HybridRetriever(vector_tool, graph_tool)

        # 5. æ„å»ºæ™ºèƒ½å¯¹è¯å¼•æ“ (ChatEngine)
        print("ğŸ¤– æ„å»º ContextChatEngine...")
        memory = ChatMemoryBuffer.from_defaults(token_limit=3000)

        chat_engine = ContextChatEngine.from_defaults(
            retriever=hybrid_retriever,
            memory=memory,
            llm=llm,
            system_prompt="""
            ä½ æ˜¯ä¸€åä¸“ä¸šçš„è®¡ç®—æœºè¯¾ç¨‹åŠ©æ•™ (EduMatrix)ã€‚
            
            ã€ä½ çš„èµ„æºã€‘ï¼š
            1. **å¯¹è¯å†å²**ï¼šç”¨æˆ·ä¹‹å‰çš„æé—®å’Œä½ ä¹‹å‰çš„å›ç­”ã€‚
            2. **èƒŒæ™¯çŸ¥è¯†**ï¼šæ£€ç´¢åˆ°çš„æ•™æåŸæ–‡(Qdrant)å’Œå›¾è°±å…³ç³»(Neo4j)ã€‚

            ã€å›ç­”ç­–ç•¥ã€‘ï¼š
            1. ğŸš¨ **æœ€é«˜ä¼˜å…ˆçº§**ï¼šå¦‚æœç”¨æˆ·é—®**â€œä½ åˆšæ‰è¯´çš„â€**ã€**â€œä¸Šä¸€æ¬¡å›ç­”â€**ç­‰å†å²ç›¸å…³é—®é¢˜ï¼Œè¯·**åŠ¡å¿…ä¼˜å…ˆåŸºäºã€å¯¹è¯å†å²ã€‘**å›ç­”ï¼Œä¸è¦é‡æ–°æ£€ç´¢æˆ–ç¼–é€ ã€‚
            2. å¯¹äºçŸ¥è¯†æ€§é—®é¢˜ï¼Œè¯·åŸºäºã€èƒŒæ™¯çŸ¥è¯†ã€‘å›ç­”ï¼Œå¹¶å°è¯•ç†æ¸…æ¦‚å¿µé—´çš„å…³ç³»ã€‚
            3. å¦‚æœèƒŒæ™¯çŸ¥è¯†ä¸è¶³ï¼Œè¯·è¯šå®å‘ŠçŸ¥ã€‚
            """
        )

        rag_engine["chat_engine"] = chat_engine
        print("âœ… å¼•æ“åˆå§‹åŒ–å®Œæˆï¼ç­‰å¾…è¯·æ±‚...")

    except Exception as e:
        print(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
        raise e

    yield 
    print("ğŸ‘‹ [Shutdown] æœåŠ¡å™¨å·²å…³é—­")

# ==========================================
# ğŸ“¡ API æ¥å£å®šä¹‰
# ==========================================

app = FastAPI(title="EduMatrix API", lifespan=lifespan)

class Message(BaseModel):
    role: str
    content: str

class ChatRequest(BaseModel):
    messages: list[Message]

@app.post("/api/chat")
async def chat_endpoint(request: ChatRequest):
    if not rag_engine:
        raise HTTPException(status_code=500, detail="Engine not initialized")
    
    # 1. è§£æè¯·æ±‚
    last_message = request.messages[-1].content
    print(f"ğŸ“© æ”¶åˆ°é—®é¢˜: {last_message}")

    # 2. å‡†å¤‡å†å²è®°å½•
    chat_history = [
        ChatMessage(role=m.role, content=m.content)
        for m in request.messages[:-1]
    ]

    # 3. è°ƒç”¨å¼•æ“ (æµå¼)
    streaming_response = rag_engine["chat_engine"].stream_chat(
        last_message,
        chat_history=chat_history,
    )

    # 4. ç”Ÿæˆæµå¼å“åº”
    def response_generator():
        # A. åå‡º AI å›ç­”
        for token in streaming_response.response_gen:
            yield token
        
        # B. åå‡ºå‚è€ƒæ¥æº (å¦‚æœæœ‰)
        if streaming_response.source_nodes:
            yield "\n\n---\n**ğŸ“š å‚è€ƒæ¥æºï¼š**\n"
            seen_sources = set()
            for node in streaming_response.source_nodes:
                # ç®€å•å»é‡å’Œæ¸…æ´—
                clean_text = node.text[:100].replace('\n', ' ')
                if clean_text not in seen_sources:
                    yield f"- {clean_text}...\n"
                    seen_sources.add(clean_text)
            
    return StreamingResponse(response_generator(), media_type="text/plain")

@app.get("/")
def read_root():
    return {"message": "EduMatrix API is running! Go to /docs for Swagger UI."}