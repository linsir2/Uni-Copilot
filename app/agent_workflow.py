import os
import httpx # [Fix 1] Use Async Client
from typing import TypedDict, List, Any
from langgraph.graph import StateGraph, END
from llama_index.core.schema import NodeWithScore, TextNode
from llama_index.core.llms import ChatMessage

class AgentState(TypedDict):
    question: str 
    original_question: str 
    chat_history: List[ChatMessage]
    retrieved_nodes: List[NodeWithScore]
    grade_status: str 
    retry_count: int
    final_response: Any
    source: str # 'local' or 'web'

def create_graph_app(retriever, llm):
    """æ„å»ºå¹¶ç¼–è¯‘ LangGraph å·¥ä½œæµ"""

    # --- è¾…åŠ©ï¼šæ„é€ è¯„åˆ† Prompt ---
    def get_grader_prompt(question, context):
        return (
            f"ä½ æ˜¯ä¸€åä¸¥æ ¼çš„è¯„åˆ†å‘˜ã€‚è¯·è¯„ä¼°ä»¥ä¸‹æ£€ç´¢åˆ°çš„æ•™æç‰‡æ®µæ˜¯å¦åŒ…å«å›ç­”ç”¨æˆ·é—®é¢˜æ‰€éœ€çš„ä¿¡æ¯ã€‚\n"
            f"é—®é¢˜: {question}\n\n"
            f"æ•™æç‰‡æ®µ:\n{context}\n\n"
            f"è¯„åˆ¤æ ‡å‡†ï¼š\n"
            f"1. ç‰‡æ®µå¿…é¡»åŒ…å«å…·ä½“çš„å®šä¹‰ã€è§£é‡Šæˆ–æ•°æ®ã€‚\n"
            f"2. å¦‚æœç‰‡æ®µåªæ˜¯æåˆ°äº†å…³é”®è¯ä½†æ²¡è§£é‡Šï¼ˆå¦‚ç›®å½•ã€ç´¢å¼•ï¼‰ï¼Œåˆ¤ä¸º noã€‚\n"
            f"3. å³ä½¿åªæœ‰éƒ¨åˆ†ç›¸å…³ï¼Œåªè¦æœ‰ç”¨ï¼Œåˆ¤ä¸º yesã€‚\n\n"
            f"è¯·åªå›å¤ 'yes' æˆ– 'no'ã€‚"
        )
    
    # --- è¾…åŠ©ï¼šå¼‚æ­¥ Tavily æœç´¢ ---
    async def tavily_search(query: str):
        api_key = os.getenv("TAVILY_API_KEY")
        if not api_key:
            print("âš ï¸ [Tavily] æœªé…ç½® API Keyï¼Œè·³è¿‡ã€‚")
            return []
        
        print(f"ğŸŒ [Tavily] æ­£åœ¨å¼‚æ­¥æœç´¢: {query}")
        payload = {
            "api_key": api_key,
            "query": query,
            "search_depth": "basic", # 'advanced' is slower, 'basic' is faster
            "include_answer": True,
            "max_results": 3,
        }

        try:
            # [Fix 1] ä½¿ç”¨ httpx è¿›è¡Œå¼‚æ­¥è¯·æ±‚ï¼Œé˜²æ­¢é˜»å¡ FastAPI
            async with httpx.AsyncClient() as client:
                resp = await client.post(
                    url="https://api.tavily.com/search",
                    json=payload,
                    timeout=10.0
                )
                resp.raise_for_status()
                data = resp.json()

            nodes = []
            # 1. Tavily ç›´æ¥ç”Ÿæˆçš„ AI ç­”æ¡ˆ
            if data.get("answer"):
                nodes.append(
                    NodeWithScore(
                        node=TextNode(
                            text=f"ã€ç½‘ç»œæ™ºèƒ½æ‘˜è¦ã€‘: {data['answer']}",
                            metadata={"file_name": "Web", "page": "AI Summary"}
                        ),
                        score=1.0,
                    )
                )
            
            # 2. å…·ä½“çš„æœç´¢ç»“æœ
            for result in data.get("results", []):
                content = f"{result['content']}\n(Source: {result['url']})"
                nodes.append(
                    NodeWithScore(
                        node=TextNode(
                            text=content,
                            metadata={"file_name": "Web", "page": "Link"}
                        ),
                        score=0.9,
                    )
                )
            
            return nodes
        
        except Exception as e:
            print(f"âŒ Tavily æœç´¢å¼‚å¸¸: {e}")
            return []

    # --- Node 1: Retrieve ---
    async def retrieve_node(state: AgentState):
        print("ğŸ” [Agent] Retrieving...")
        question = state["question"]
        chat_history = state.get("chat_history", [])

        # Query Rewrite (Simplification)
        # Only rewrite if we have history and it's not a retry
        search_query = question
        if chat_history and state.get("retry_count", 0) == 0:
            history_txt = "\n".join([f"{m.role}: {m.content}" for m in chat_history[-2:]])
            prompt = (
                f"åŸºäºå¯¹è¯å†å²ï¼Œå°†ç”¨æˆ·æœ€æ–°çš„é—®é¢˜æ”¹å†™ä¸ºç‹¬ç«‹çš„æœç´¢å…³é”®è¯ã€‚\n"
                f"å†å²: {history_txt}\né—®é¢˜: {question}\n"
                f"è¾“å‡º(ä»…å…³é”®è¯):"
            )
            res = await llm.acomplete(prompt)
            search_query = res.text.strip()
            print(f"   -> æ”¹å†™ Query: {search_query}")

        nodes = await retriever.aretrieve(search_query)
        print(f"   -> æ£€ç´¢åˆ° {len(nodes)} æ¡")
        return {"retrieved_nodes": nodes, "source": "local", "question": search_query}
    
    # --- Node 2: Grade ---
    async def grade_node(state: AgentState):
        question = state["question"]
        nodes = state["retrieved_nodes"]

        if not nodes:
            return {"grade_status": "no"}
            
        # é¢„è§ˆå‰3æ¡å†…å®¹ç”¨äºè¯„åˆ†
        context_preview = "\n".join([n.node.get_content()[:200] for n in nodes[:3]])
        prompt = get_grader_prompt(question, context_preview)

        response = await llm.acomplete(prompt)
        score = response.text.strip().lower()
        status = "yes" if "yes" in score else "no"
        
        print(f"âš–ï¸ [Agent] è¯„åˆ†: {status}")
        return {"grade_status": status}
    
    # --- Node 3: Rewrite (Loop) ---
    async def rewrite_node(state: AgentState):
        print("ğŸ”„ [Agent] é‡å†™æŸ¥è¯¢è¯...")
        question = state["question"]
        prompt = f"ç”¨æˆ·é—®é¢˜ '{question}' åœ¨æ•™æä¸­æœªæœåˆ°ã€‚è¯·å°è¯•æå–æ ¸å¿ƒå®ä½“è¯ï¼Œå»é™¤ä¿®é¥°è¯ï¼Œé‡å†™æŸ¥è¯¢ã€‚"
        res = await llm.acomplete(prompt)
        new_q = res.text.strip()
        print(f"   -> æ–° Query: {new_q}")
        
        return {
            "question": new_q, 
            "retry_count": state.get("retry_count", 0) + 1
        }
    
    # --- Node 4: Web Search ---
    async def web_search_node(state: AgentState):
        print("ğŸŒ [Agent] å¯åŠ¨ Web Search...")
        nodes = await tavily_search(state["original_question"])
        return {"retrieved_nodes": nodes, "source": "web"}
    
    # --- Node 5: Generate (With Citations) ---
    async def generate_node(state: AgentState):
        print("âœï¸ [Agent] Generating...")
        nodes = state["retrieved_nodes"]
        source_type = state.get("source", "local")
        
        # [Fix 2 & 3] Context Injection Logic
        context_lines = []
        for i, n in enumerate(nodes):
            # Safe Metadata Access
            meta = n.node.metadata or {}
            file = meta.get("file_name", "æ•™æ")
            page = meta.get("page", "?")
            
            # Construct Citation Tag
            if source_type == "web":
                citation = "[Web]"
            else:
                citation = f"[{file} P{page}]"
            
            # Inject into text so LLM sees it
            text = n.node.get_content()
            context_lines.append(f"å¼•ç”¨æ¥æº {citation}:\n{text}\n")

        context_str = "\n".join(context_lines)
        
        system_prompt = (
            f"ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è®¡ç®—æœºåŠ©æ•™ã€‚åŸºäºæä¾›çš„{source_type}èµ„æ–™å›ç­”é—®é¢˜ã€‚\n"
            f"ã€å¼•ç”¨è§„èŒƒã€‘ï¼š\n"
            f"1. å‡¡æ˜¯å¼•ç”¨äº†èµ„æ–™é‡Œçš„è§‚ç‚¹æˆ–æ•°æ®ï¼Œå¿…é¡»åœ¨å¥å°¾åŠ ä¸Šæ¥æºæ ‡ç­¾ï¼Œå¦‚ [è®¡ç®—æœºç»„æˆ.pdf P12]ã€‚\n"
            f"2. æ¥æºæ ‡ç­¾æˆ‘å·²ç»éƒ½åœ¨èµ„æ–™é‡Œç»™ä½ å†™å¥½äº†ï¼Œç›´æ¥æŠ„ä¸‹æ¥ã€‚\n"
            f"3. å¦‚æœèµ„æ–™é‡Œæ²¡æœ‰æåŠï¼Œä¸è¦ç¼–é€ ã€‚\n"
            f"4. ä¿æŒå›ç­”ç®€æ´ã€é€»è¾‘æ¸…æ™°ã€‚"
        )
        
        sys_msg = ChatMessage(role="system", content=system_prompt)
        user_msg = ChatMessage(role="user", content=f"èµ„æ–™ï¼š\n{context_str}\n\né—®é¢˜ï¼š{state['original_question']}")
        
        # Return the stream iterator directly
        response_stream = await llm.astream_chat([sys_msg, user_msg])
        return {"final_response": response_stream}

    # --- Node 6: Fallback ---
    async def apologize_node(state: AgentState):
        return {"final_response": "æŠ±æ­‰ï¼Œæˆ‘åœ¨æœ¬åœ°æ•™æå’Œç½‘ç»œä¸Šéƒ½æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚"}

    # --- Build Graph ---
    workflow = StateGraph(AgentState)

    workflow.add_node("retrieve", retrieve_node)
    workflow.add_node("grade", grade_node)
    workflow.add_node("rewrite", rewrite_node)
    workflow.add_node("web_search", web_search_node)
    workflow.add_node("generate", generate_node)
    workflow.add_node("apologize", apologize_node)

    workflow.set_entry_point("retrieve")
    workflow.add_edge("retrieve", "grade")
    workflow.add_edge("rewrite", "retrieve")

    # Conditional Logic
    def decide_local(state):
        if state["grade_status"] == "yes":
            return "generate"
        elif state["retry_count"] < 1: # Retry once
            return "rewrite"
        else:
            return "web_search"

    workflow.add_conditional_edges(
        "grade", 
        decide_local,
        {
            "generate": "generate",
            "rewrite": "rewrite",
            "web_search": "web_search"
        }
    )
    
    workflow.add_edge("web_search", "generate") # Simplify: Trust web search for now
    workflow.add_edge("generate", END)
    workflow.add_edge("apologize", END)

    return workflow.compile()