from typing import TypedDict, List, Any
from langgraph.graph import StateGraph, END
from llama_index.core.schema import NodeWithScore, TextNode
from llama_index.core.llms import ChatMessage
import os
import requests


class AgentState(TypedDict):
    question: str # å½“å‰çš„é—®é¢˜ï¼ˆå¯èƒ½æ˜¯è¢«é‡å†™è¿‡çš„ç‰ˆæœ¬ï¼‰
    original_question: str # æœ€å¼€å§‹ç”¨æˆ·é—®çš„é—®é¢˜ï¼ˆæ²¡è¢«llmä¿®æ”¹è¿‡çš„ç‰ˆæœ¬ï¼‰ï¼›ç”¨äºæœ€ållmç”Ÿæˆå›ç­”
    chat_history: List[ChatMessage]
    retrieved_nodes: List[NodeWithScore]
    grade_status: str # "yes" or "no"
    retry_count: int
    final_response: Any
    source: str # local or web

def create_graph_app(retriever, llm):
    """æ„å»ºå¹¶ç¼–è¯‘ LangGraph å·¥ä½œæµ"""
    # è¾…åŠ©å·¥å…·ï¼šæ„é€ è¯„åˆ†prompt
    def get_grader_prompt(question, context):
        return (
            f"ä½ æ˜¯ä¸€åè¯„åˆ†å‘˜ã€‚è¯·è¯„ä¼°ä»¥ä¸‹æ£€ç´¢åˆ°çš„æ•™æç‰‡æ®µæ˜¯å¦ä¸ç”¨æˆ·çš„é—®é¢˜ç›¸å…³ã€‚\n"
            f"é—®é¢˜: {question}\n\n"
            f"æ•™æç‰‡æ®µ:\n{context}\n\n"
            f"å¦‚æœç‰‡æ®µåŒ…å«èƒ½å›ç­”é—®é¢˜çš„å…³é”®è¯æˆ–è¯­ä¹‰ï¼Œè¯·å›å¤ 'yes'ï¼Œå¦åˆ™å›å¤ 'no'ã€‚\n"
            f"åªå›å¤ 'yes' æˆ– 'no'ï¼Œä¸è¦åºŸè¯ã€‚"
        )
    
    def tavily_search(query: str):
        api_key = os.getenv("TAVILY_API_KEY")
        if not api_key:
            print("âš ï¸ æœªé…ç½® TAVILY_API_KEYï¼Œè·³è¿‡è”ç½‘æœç´¢ã€‚")
            return []
        
        print(f"ğŸŒ [Tavily] æ­£åœ¨æœç´¢äº’è”ç½‘: {query}")
        payload = {
            "api_key": api_key,
            "query": query,
            "search_depth": "advanced",
            "include_answer": True,
            "max_results": 3,
        }

        try:
            response = requests.post(
                url=os.getenv("TAVILY_BASE_URL") or "",
                json=payload,
                timeout=20,
            )
            data = response.json()

            nodes = []

            if data.get("answer"):
                nodes.append(
                    NodeWithScore(
                        node=TextNode(text=f"ã€Tavily æ™ºèƒ½æ‘˜è¦ã€‘: {data['answer']}"),
                        score=1.0,
                    )
                )
            
            for result in data.get("results", []):
                content = f"ã€æ¥æº: {result['title']}ã€‘\n{result['content']}\n(URL: {result['url']})"
                nodes.append(
                    NodeWithScore(
                        node=TextNode(text=content),
                        score=0.9,
                    )
                )
            
            return nodes
        
        except Exception as e:
            print(f"âŒ Tavily æœç´¢å¤±è´¥: {e}")
            return []

    async def retrieve_node(state: AgentState):
        print("ğŸ” [Node] Retrieving...")
        question = state["question"]
        chat_history = state.get("chat_history", [])

        # å¦‚æœæœ‰å†å²è®°å½•ï¼Œåˆ™è¿›è¡Œ Query å‹ç¼©/æ”¹å†™
        if chat_history:
            history_str = "\n".join([f"{m.role}: {m.content}" for m in chat_history[-3:]]) # å–æœ€è¿‘3è½®
            rewrite_prompt = (
                f"ç»“åˆä»¥ä¸‹å¯¹è¯å†å²ï¼Œå°†ç”¨æˆ·æœ€æ–°çš„é—®é¢˜æ”¹å†™ä¸ºä¸€ä¸ªç‹¬ç«‹ã€å®Œæ•´çš„æœç´¢æŒ‡ä»¤ã€‚\n"
                f"å¯¹è¯å†å²:\n{history_str}\n"
                f"å½“å‰é—®é¢˜: {question}\n"
                f"æ”¹å†™åçš„å®Œæ•´é—®é¢˜ï¼ˆåªéœ€è¾“å‡ºæ”¹å†™åçš„æ–‡æœ¬ï¼‰:"
            )
            # è°ƒç”¨ llm è¿›è¡Œæ”¹å†™
            rewrite_res = await llm.acomplete(rewrite_prompt)
            search_query = rewrite_res.text.strip()
            print(f"   -> è½¬æ¢åçš„ Query: {search_query}")
        else:
            search_query = question

        # ä½¿ç”¨è½¬æ¢åçš„ search_query è¿›è¡Œæ£€ç´¢
        nodes = await retriever.aretrieve(search_query)

        print(f"   -> æ£€ç´¢åˆ° {len(nodes)} æ¡ç›¸å…³ç‰‡æ®µ")
        return {"retrieved_nodes": nodes, "source": "local", "question": search_query}
    
    async def grade_node(state: AgentState):
        print("âš–ï¸ [Agent] æ­£åœ¨è¯„ä¼°èµ„æ–™è´¨é‡...")
        question = state["question"]
        nodes = state["retrieved_nodes"]

        # 1. å¦‚æœæ ¹æœ¬æ²¡æœåˆ°ï¼Œç›´æ¥åˆ¤æ­»åˆ‘
        if not nodes:
            return {"grade_status": "no"}
            
        # 2. æ„é€ ä¸Šä¸‹æ–‡ä¾› LLM åˆ¤æ–­
        # å–å‰ 3 ä¸ªç‰‡æ®µçš„å†…å®¹æ‹¼æ¥ï¼Œé¿å… token çˆ†ç‚¸
        context_preview = "\n".join([n.get_content()[:200] for n in nodes[:3]])

        prompt = get_grader_prompt(question, context_preview)

        # 3. è°ƒç”¨ LLM è¿›è¡ŒäºŒåˆ†ç±»
        # è¿™é‡Œç”¨ complete è€Œä¸æ˜¯ streamï¼Œå› ä¸ºæˆ‘ä»¬è¦æ‹¿ç»“æœåšåˆ¤æ–­
        response = await llm.acomplete(prompt)
        score = response.text.strip().lower()

        if "yes" in score:
            status = "yes"
        else:
            status = "no"
        
        print(f"   -> è¯„åˆ†ç»“æœ: {status}")
        return {"grade_status": status}
    
    async def rewrite_node(state: AgentState):
        print("ğŸ”„ [Agent] èµ„æ–™ä¸å…¨ï¼Œæ­£åœ¨é‡å†™æŸ¥è¯¢è¯...")
        question = state["question"]

        prompt = (
            f"ç”¨æˆ·çš„é—®é¢˜æ˜¯ï¼š'{question}'ã€‚\n"
            f"ç›®å‰çš„æ£€ç´¢ç»“æœä¸ä½³ã€‚è¯·æ ¹æ®è¯­ä¹‰æŠŠè¿™ä¸ªé—®é¢˜é‡å†™å¾—æ›´ç²¾å‡†ï¼Œæˆ–è€…æ˜¯æå–æ ¸å¿ƒå…³é”®è¯ã€‚\n"
            f"åªè¾“å‡ºé‡å†™åçš„é—®é¢˜ï¼Œä¸è¦è§£é‡Šã€‚"
        )

        response = await llm.acomplete(prompt)
        new_question = response.text.strip()

        print(f"   -> æ–°é—®é¢˜: {new_question}")
        
        # æ›´æ–°é—®é¢˜ï¼Œå¹¶å¢åŠ è®¡æ•°å™¨
        return {
            "question": new_question, 
            "retry_count": state.get("retry_count", 0) + 1
        }
    
    async def web_search_node(state: AgentState):
        print("ğŸŒ [Agent] æœ¬åœ°å½»åº•æ²¡æˆäº†ï¼Œå¯åŠ¨ Deep Research (Tavily)...")
        query = state["original_question"]

        web_nodes = tavily_search(query)

        print(f"   -> è”ç½‘è·å–äº† {len(web_nodes)} æ¡ä¿¡æ¯")
        # è¦†ç›–æ‰ä¹‹å‰çš„æœ¬åœ°ç»“æœï¼Œå› ä¸ºæœ¬åœ°çš„åæ­£ä¹Ÿæ²¡ç”¨
        return {"retrieved_nodes": web_nodes, "source": "web"}
    
    async def grade_web_node(state: AgentState):
        print("âš–ï¸ [Agent] æ­£åœ¨å®¡æ ¸ç½‘ç»œæœç´¢ç»“æœ...")

        query = state["original_question"]
        nodes = state["retrieved_nodes"]

        if not nodes:
            print("   -> ç½‘ç»œæœç´¢ä¸ºç©º")
            return {"grade_status": "no"}
        
        context_preview = "\n".join([n.get_content()[:300] for n in nodes[:3]])
        prompt = get_grader_prompt(query, context_preview)

        response = await llm.acomplete(prompt)
        status = "yes" if "yes" in response.text.strip().lower() else "no"
        print(f"-> ç½‘ç»œè¯„åˆ†ç»“æœ: {status}")
        return {"grade_status": status}
    
    async def generate_node(state: AgentState):
        print("âœï¸ [Agent] æ­£åœ¨ç»„ç»‡è¯­è¨€ç”Ÿæˆå›ç­” (å«å¼•ç”¨)...")
        source_type = state.get("source", "local")
        nodes = state["retrieved_nodes"]
        
        # 1. ç²¾ç»†åŒ–æ„å»º Context (æ ¸å¿ƒä¿®æ”¹ç‚¹) ğŸ†•
        context_list = []
        for n in nodes:
            # å°è¯•è·å–é¡µç  (LlamaParse é€šå¸¸å­˜ä¸º 'page_label')
            # å¦‚æœæ˜¯ Web æœç´¢ç»“æœï¼Œmetadata å¯èƒ½ä¸ºç©ºï¼Œæˆ‘ä»¬åšä¸ªå…¼å®¹
            meta = n.metadata or {}
            page = meta.get("page_label", "æœªçŸ¥é¡µ")
            file_name = meta.get("file_name", "æ•™æ")
            
            # æ ¹æ®æ¥æºç±»å‹ï¼Œç”Ÿæˆä¸åŒçš„å¼•ç”¨æ ‡ç­¾
            if source_type == "web":
                # Web ç»“æœé€šå¸¸åœ¨ TextNode é‡Œå·²ç»åŒ…å« URLï¼Œè¿™é‡Œå¯ä»¥ç®€åŒ–
                source_tag = "[Web]" 
            else:
                # æœ¬åœ°ç»“æœï¼š[æ•™æåç§° P12]
                source_tag = f"[{file_name} P{page}]"
            
            # æ‹¼æ¥åˆ°æ–‡æœ¬å‰ï¼Œè®© LLM çŸ¥é“è¿™æ®µè¯çš„å‡ºå¤„
            text = n.get_content()
            context_list.append(f"---æ¥æº {source_tag}---\n{text}")

        context_str = "\n\n".join(context_list)
        
        # 2. æ„é€ å¸¦â€œå¼ºåˆ¶å¼•ç”¨æŒ‡ä»¤â€çš„ Prompt ğŸ†•
        system_prompt = (
            f"ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„è®¡ç®—æœºè¯¾ç¨‹åŠ©æ•™ã€‚è¯·åŸºäº{source_type}èµ„æ–™å›ç­”é—®é¢˜ã€‚\n"
            "ã€ä¸¥æ ¼å¼•ç”¨è¦æ±‚ã€‘ï¼š\n"
            "1. ä½ çš„å›ç­”å¿…é¡»å»ºç«‹åœ¨å‚è€ƒèµ„æ–™çš„åŸºç¡€ä¸Šï¼Œä¸è¦ç¼–é€ ï¼Œä¸èƒ½è‡ªå·±ç¼–å†™ï¼Œåªèƒ½ä»ç»™å‡ºçš„æ–‡æœ¬ä¸­æ€»ç»“å›ç­”ã€‚\n"
            "2. **å…³é”®æ­¥éª¤**ï¼šåœ¨å¼•ç”¨æŸä¸ªçŸ¥è¯†ç‚¹æ—¶ï¼Œè¯·åœ¨å¥å°¾æ ‡æ³¨å…¶æ¥æºé¡µç ï¼Œæ ¼å¼ä¸¥æ ¼ä¸º [æ•™æ Pxx] æˆ– [Web]ã€‚\n"
            "   - é”™è¯¯ç¤ºä¾‹ï¼š...æ­»é”çš„å®šä¹‰(P12)ã€‚\n"
            "   - æ­£ç¡®ç¤ºä¾‹ï¼š...æ­»é”æ˜¯æŒ‡ä¸¤ä¸ªè¿›ç¨‹äº’ç›¸ç­‰å¾… [è®¡ç®—æœºç»„æˆåŸç†.pdf P12]ã€‚\n"
            "3. å¦‚æœä¸åŒæ®µè½æ¥è‡ªä¸åŒé¡µç ï¼Œè¯·åˆ†åˆ«æ ‡æ³¨ã€‚\n"
        )
        
        system_msg = ChatMessage(role="system", content=system_prompt)
        # ä½¿ç”¨åŸå§‹é—®é¢˜ original_question ä»¥ä¿è¯å‡†ç¡®æ€§
        user_msg = ChatMessage(role="user", content=f"å‚è€ƒèµ„æ–™ï¼š\n{context_str}\n\nç”¨æˆ·é—®é¢˜ï¼š{state['original_question']}")
        
        response_stream = await llm.astream_chat([system_msg] + state.get("chat_history", []) + [user_msg])
        
        return {"final_response": response_stream}
    
    # å½“å…¨ç½‘éƒ½æœä¸åˆ°æ—¶ï¼Œä½“é¢åœ°ç»“æŸ
    async def apologize_node(state: AgentState):
        print("ğŸ›‘ [Agent] å½»åº•æ”¾å¼ƒï¼Œæ‰§è¡Œ Fallback...")
        text = "éå¸¸æŠ±æ­‰ï¼Œæˆ‘åœ¨æœ¬åœ°æ•™æå’Œäº’è”ç½‘ä¸Šéƒ½æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚è¿™å¯èƒ½æ˜¯ä¸€ä¸ªéå¸¸ç”Ÿåƒ»çš„çŸ¥è¯†ç‚¹ï¼Œå»ºè®®æ‚¨æŸ¥é˜…æ›´ä¸“ä¸šçš„å­¦æœ¯æ–‡çŒ®ã€‚"
        # ç›´æ¥è¿”å›å­—ç¬¦ä¸²ï¼Œmain.py ä¹Ÿèƒ½å¤„ç†
        return {"final_response": text}
    
    workflow = StateGraph(AgentState)

    workflow.add_node("retrieve", retrieve_node)
    workflow.add_node("grade", grade_node)
    workflow.add_node("rewrite", rewrite_node)
    workflow.add_node("web_search", web_search_node)
    workflow.add_node("grade_web", grade_web_node)
    workflow.add_node("generate", generate_node)
    workflow.add_node("apologize", apologize_node)

    workflow.set_entry_point("retrieve")

    workflow.add_edge("retrieve", "grade")
    workflow.add_edge("rewrite", "retrieve")
    workflow.add_edge("web_search", "grade_web")

    # æ¡ä»¶è¾¹ 1: æœ¬åœ°è¯„åˆ†å
    def decide_local(state):
        if state["grade_status"] == "yes":
            return "generate"
        elif state.get("retry_count", 0) < 1:
            return "rewrite"
        else:
            return "web_search"
    
    workflow.add_conditional_edges(
        "grade", decide_local,
        {
            "generate": "generate",
            "rewrite": "rewrite",
            "web_search": "web_search",
        }
    )

    # ğŸ†• æ¡ä»¶è¾¹ 2: ç½‘ç»œè¯„åˆ†å
    def decide_web(state):
        if state["grade_status"] == "yes":
            return "generate" # ç½‘ç»œç»“æœé è°±ï¼Œå»ç”Ÿæˆ
        else:
            return "apologize" # ç½‘ç»œç»“æœä¹Ÿæ˜¯åƒåœ¾ï¼Œå»é“æ­‰
    
    workflow.add_conditional_edges(
        "grade_web", decide_web,
        {
            "generate": "generate",
            "apologize": "apologize",
        }
    )

    workflow.add_edge("apologize", END)
    workflow.add_edge("generate", END)

    app = workflow.compile()
    return app