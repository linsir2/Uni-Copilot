from typing import TypedDict, List, Any
from langgraph.graph import StateGraph, END
from llama_index.core.schema import NodeWithScore, TextNode
from llama_index.core.llms import ChatMessage
import os
import requests


class AgentState(TypedDict):
    question: str # å½“å‰çš„é—®é¢˜ï¼ˆå¯èƒ½æ˜¯è¢«é‡å†™è¿‡çš„ç‰ˆæœ¬ï¼‰
    original_question: str # æœ€å¼€å§‹ç”¨æˆ·é—®çš„é—®é¢˜ï¼ˆæ²¡è¢«llmä¿®æ”¹è¿‡çš„ç‰ˆæœ¬ï¼‰ï¼›ç”¨äºæœ€ållmç”Ÿæˆå›ç­”
    chat_history: List[ChatMessage]
    retrieved_nodes: List[NodeWithScore]
    grade_status: str # "yes" or "no"
    retry_count: int
    final_response: Any
    source: str # local or web

def create_graph_app(retriever, llm):
    """æ„å»ºå¹¶ç¼–è¯‘ LangGraph å·¥ä½œæµ"""
    # è¾…åŠ©å·¥å…·ï¼šæ„é€ è¯„åˆ†prompt
    def get_grader_prompt(question, context):
        return (
            f"ä½ æ˜¯ä¸€åè¯„åˆ†å‘˜ã€‚è¯·è¯„ä¼°ä»¥ä¸‹æ£€ç´¢åˆ°çš„æ•™æç‰‡æ®µæ˜¯å¦ä¸ç”¨æˆ·çš„é—®é¢˜ç›¸å…³ã€‚\n"
            f"é—®é¢˜: {question}\n\n"
            f"æ•™æç‰‡æ®µ:\n{context}\n\n"
            f"å¦‚æœç‰‡æ®µåŒ…å«èƒ½å›ç­”é—®é¢˜çš„å…³é”®è¯æˆ–è¯­ä¹‰ï¼Œè¯·å›å¤ 'yes'ï¼Œå¦åˆ™å›å¤ 'no'ã€‚\n"
            f"åªå›å¤ 'yes' æˆ– 'no'ï¼Œä¸è¦åºŸè¯ã€‚"
        )
    
    def tavily_search(query: str):
        api_key = os.getenv("TAVILY_API_KEY")
        if not api_key:
            print("âš ï¸ æœªé…ç½® TAVILY_API_KEYï¼Œè·³è¿‡è”ç½‘æœç´¢ã€‚")
            return []
        
        print(f"ğŸŒ [Tavily] æ­£åœ¨æœç´¢äº’è”ç½‘: {query}")
        payload = {
            "api_key": api_key,
            "query": query,
            "search_depth": "advanced",
            "include_answer": True,
            "max_results": 3,
        }

        try:
            response = requests.post(
                url=os.getenv("TAVILY_BASE_URL") or "",
                json=payload,
                timeout=20,
            )
            data = response.json()

            nodes = []

            if data.get("answer"):
                nodes.append(
                    NodeWithScore(
                        node=TextNode(text=f"ã€Tavily æ™ºèƒ½æ‘˜è¦ã€‘: {data['answer']}"),
                        score=1.0,
                    )
                )
            
            for result in data.get("results", []):
                content = f"ã€æ¥æº: {result['title']}ã€‘\n{result['content']}\n(URL: {result['url']})"
                nodes.append(
                    NodeWithScore(
                        node=TextNode(text=content),
                        score=0.9,
                    )
                )
            
            return nodes
        
        except Exception as e:
            print(f"âŒ Tavily æœç´¢å¤±è´¥: {e}")
            return []

    async def retrieve_node(state: AgentState):
        print("ğŸ” [Node] Retrieving...")
        question = state["question"]
    
        nodes = await retriever.aretrieve(question)

        print(f"   -> æ£€ç´¢åˆ° {len(nodes)} æ¡ç›¸å…³ç‰‡æ®µ")
        return {"retrieved_nodes": nodes, "source": "local"}
    
    async def grade_node(state: AgentState):
        print("âš–ï¸ [Agent] æ­£åœ¨è¯„ä¼°èµ„æ–™è´¨é‡...")
        question = state["question"]
        nodes = state["retrieved_nodes"]

        # 1. å¦‚æœæ ¹æœ¬æ²¡æœåˆ°ï¼Œç›´æ¥åˆ¤æ­»åˆ‘
        if not nodes:
            return {"grade_status": "no"}
            
        # 2. æ„é€ ä¸Šä¸‹æ–‡ä¾› LLM åˆ¤æ–­
        # å–å‰ 3 ä¸ªç‰‡æ®µçš„å†…å®¹æ‹¼æ¥ï¼Œé¿å… token çˆ†ç‚¸
        context_preview = "\n".join([n.get_content()[:200] for n in nodes[:3]])

        prompt = get_grader_prompt(question, context_preview)

        # 3. è°ƒç”¨ LLM è¿›è¡ŒäºŒåˆ†ç±»
        # è¿™é‡Œç”¨ complete è€Œä¸æ˜¯ streamï¼Œå› ä¸ºæˆ‘ä»¬è¦æ‹¿ç»“æœåšåˆ¤æ–­
        response = await llm.acomplete(prompt)
        score = response.text.strip().lower()

        if "yes" in score:
            status = "yes"
        else:
            status = "no"
        
        print(f"   -> è¯„åˆ†ç»“æœ: {status}")
        return {"grade_status": status}
    
    async def rewrite_node(state: AgentState):
        print("ğŸ”„ [Agent] èµ„æ–™ä¸å…¨ï¼Œæ­£åœ¨é‡å†™æŸ¥è¯¢è¯...")
        question = state["question"]

        prompt = (
            f"ç”¨æˆ·çš„é—®é¢˜æ˜¯ï¼š'{question}'ã€‚\n"
            f"ç›®å‰çš„æ£€ç´¢ç»“æœä¸ä½³ã€‚è¯·æ ¹æ®è¯­ä¹‰æŠŠè¿™ä¸ªé—®é¢˜é‡å†™å¾—æ›´ç²¾å‡†ï¼Œæˆ–è€…æ˜¯æå–æ ¸å¿ƒå…³é”®è¯ã€‚\n"
            f"åªè¾“å‡ºé‡å†™åçš„é—®é¢˜ï¼Œä¸è¦è§£é‡Šã€‚"
        )

        response = await llm.acomplete(prompt)
        new_question = response.text.strip()

        print(f"   -> æ–°é—®é¢˜: {new_question}")
        
        # æ›´æ–°é—®é¢˜ï¼Œå¹¶å¢åŠ è®¡æ•°å™¨
        return {
            "question": new_question, 
            "retry_count": state.get("retry_count", 0) + 1
        }
    
    async def web_search_node(state: AgentState):
        print("ğŸŒ [Agent] æœ¬åœ°å½»åº•æ²¡æˆäº†ï¼Œå¯åŠ¨ Deep Research (Tavily)...")
        query = state["original_question"]

        web_nodes = tavily_search(query)

        print(f"   -> è”ç½‘è·å–äº† {len(web_nodes)} æ¡ä¿¡æ¯")
        # è¦†ç›–æ‰ä¹‹å‰çš„æœ¬åœ°ç»“æœï¼Œå› ä¸ºæœ¬åœ°çš„åæ­£ä¹Ÿæ²¡ç”¨
        return {"retrieved_nodes": web_nodes, "source": "web"}
    
    async def grade_web_node(state: AgentState):
        print("âš–ï¸ [Agent] æ­£åœ¨å®¡æ ¸ç½‘ç»œæœç´¢ç»“æœ...")

        query = state["original_question"]
        nodes = state["retrieved_nodes"]

        if not nodes:
            print("   -> ç½‘ç»œæœç´¢ä¸ºç©º")
            return {"grade_status": "no"}
        
        context_preview = "\n".join([n.get_content()[:300] for n in nodes[:3]])
        prompt = get_grader_prompt(query, context_preview)

        response = await llm.acomplete(prompt)
        status = "yes" if "yes" in response.text.strip().lower() else "no"
        print(f"-> ç½‘ç»œè¯„åˆ†ç»“æœ: {status}")
        return {"grade_status": status}
    
    async def generate_node(state: AgentState):
        print("âœï¸ [Agent] æ­£åœ¨ç»„ç»‡è¯­è¨€ç”Ÿæˆå›ç­” (Async)...")
        final_question = state["original_question"]
        nodes = state["retrieved_nodes"]
        history = state.get("chat_history", [])

        # 1. æ‹¼å‡‘ä¸Šä¸‹æ–‡
        context_str = "\n\n".join([f"---ç‰‡æ®µ---\n{n.get_content()}" for n in nodes])
        
        # 2. æ„é€  Prompt
        system_msg = ChatMessage(role="system", content="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è®¡ç®—æœºè¯¾ç¨‹åŠ©æ•™ã€‚è¯·æ ¹æ®æä¾›çš„æ•™æç‰‡æ®µå›ç­”é—®é¢˜ã€‚å¦‚æœç‰‡æ®µä¸­æ²¡æœ‰ç­”æ¡ˆï¼Œè¯·è¯šå®å‘ŠçŸ¥ã€‚")
        user_msg = ChatMessage(role="user", content=f"å‚è€ƒèµ„æ–™ï¼š\n{context_str}\n\nç”¨æˆ·é—®é¢˜ï¼š{final_question}")
        
        messages = [system_msg] + history + [user_msg]

        response_stream = await llm.astream_chat(messages)

        return {"final_response": response_stream}
    
    # å½“å…¨ç½‘éƒ½æœä¸åˆ°æ—¶ï¼Œä½“é¢åœ°ç»“æŸ
    async def apologize_node(state: AgentState):
        print("ğŸ›‘ [Agent] å½»åº•æ”¾å¼ƒï¼Œæ‰§è¡Œ Fallback...")
        text = "éå¸¸æŠ±æ­‰ï¼Œæˆ‘åœ¨æœ¬åœ°æ•™æå’Œäº’è”ç½‘ä¸Šéƒ½æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚è¿™å¯èƒ½æ˜¯ä¸€ä¸ªéå¸¸ç”Ÿåƒ»çš„çŸ¥è¯†ç‚¹ï¼Œå»ºè®®æ‚¨æŸ¥é˜…æ›´ä¸“ä¸šçš„å­¦æœ¯æ–‡çŒ®ã€‚"
        # ç›´æ¥è¿”å›å­—ç¬¦ä¸²ï¼Œmain.py ä¹Ÿèƒ½å¤„ç†
        return {"final_response": text}
    
    workflow = StateGraph(AgentState)

    workflow.add_node("retrieve", retrieve_node)
    workflow.add_node("grade", grade_node)
    workflow.add_node("rewrite", rewrite_node)
    workflow.add_node("web_search", web_search_node)
    workflow.add_node("grade_web", grade_web_node)
    workflow.add_node("generate", generate_node)
    workflow.add_node("apologize", apologize_node)

    workflow.set_entry_point("retrieve")

    workflow.add_edge("retrieve", "grade")
    workflow.add_edge("rewrite", "retrieve")
    workflow.add_edge("web_search", "grade_web")

    # æ¡ä»¶è¾¹ 1: æœ¬åœ°è¯„åˆ†å
    def decide_local(state):
        if state["grade_status"] == "yes":
            return "generate"
        elif state.get("retry_count", 0) < 1:
            return "rewrite"
        else:
            return "web_search"
    
    workflow.add_conditional_edges(
        "grade", decide_local,
        {
            "generate": "generate",
            "rewrite": "rewrite",
            "web_search": "web_search",
        }
    )

    # ğŸ†• æ¡ä»¶è¾¹ 2: ç½‘ç»œè¯„åˆ†å
    def decide_web(state):
        if state["grade_status"] == "yes":
            return "generate" # ç½‘ç»œç»“æœé è°±ï¼Œå»ç”Ÿæˆ
        else:
            return "apologize" # ç½‘ç»œç»“æœä¹Ÿæ˜¯åƒåœ¾ï¼Œå»é“æ­‰
    
    workflow.add_conditional_edges(
        "grade_web", decide_web,
        {
            "generate": "generate",
            "apologize": "apologize",
        }
    )

    workflow.add_edge("apologize", END)
    workflow.add_edge("generate", END)

    app = workflow.compile()
    return app