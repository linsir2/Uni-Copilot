import os
import json
import pandas as pd
from qdrant_client import QdrantClient
from langchain_core.documents import Document as LCDocument
from ragas.testset import TestsetGenerator
from ragas.run_config import RunConfig
from langchain_community.embeddings import DashScopeEmbeddings
from langchain_openai import ChatOpenAI
from dotenv import load_dotenv
from pathlib import Path

load_dotenv()

# 1. é…ç½®
COLLECTION_NAME = "edu_matrix_chunks"
QDRANT_URL = os.getenv("QDRANT_URL", "http://localhost:6333")
TESTSET_SIZE = 15  # ç”Ÿæˆå¤šå°‘é“é¢˜

def fetch_docs_from_qdrant():
    print(f"ğŸ”Œ è¿æ¥æ•°æ®åº“: {QDRANT_URL} ...")
    client = QdrantClient(url=QDRANT_URL)
    
    # ä½¿ç”¨ Scroll éå†æ•°æ®
    # è¿™é‡Œæˆ‘ä»¬å–å‰ 300 ä¸ª chunk ä½œä¸ºå‡ºé¢˜ç´ æï¼ˆé¿å… Token çˆ†ç‚¸ï¼‰
    # å¦‚æœæ˜¯ç”Ÿäº§ç¯å¢ƒï¼Œå»ºè®®éšæœºé‡‡æ ·
    records, _ = client.scroll(
        collection_name=COLLECTION_NAME,
        limit=200, 
        with_payload=True,
        with_vectors=False
    )
    
    docs = []
    print(f"ğŸ“¥ ä» Qdrant è¯»å–åˆ° {len(records)} ä¸ªç‰‡æ®µ...")

    for r in records:
        payload = r.payload
        
        # ğŸš« å…³é”®è¿‡æ»¤ï¼šä¸è¦åŸºäºâ€œå…¨æ–‡æ‘˜è¦â€å‡ºé¢˜
        # æ‘˜è¦åŒ…å«å…¨ä¹¦å†…å®¹ï¼Œå‡ºçš„é¢˜å¤ªå®è§‚ï¼Œå®¹æ˜“å¯¼è‡´æ£€ç´¢è¯„ä¼°ä¸å‡†
        if payload.get("is_global_summary") == "true":
            continue

        # æå–å†…å®¹ (å…¼å®¹ä¸åŒå­˜å‚¨æ ¼å¼)
        content = payload.get("text")
        if not content and "_node_content" in payload:
            try:
                content = json.loads(payload["_node_content"]).get("text")
            except:
                pass
        
        if content:
            # è½¬æ¢ä¸º LangChain Document ç»™ Ragas ç”¨
            docs.append(LCDocument(
                page_content=content,
                metadata={
                    "filename": payload.get("file_name", "unknown"),
                    "page_label": payload.get("page_label", "?")
                }
            ))
    
    print(f"âœ… ç­›é€‰åæœ‰æ•ˆå‡ºé¢˜ç‰‡æ®µ: {len(docs)} ä¸ª")
    return docs

def main():
    # 1. è·å–çœŸå®æ•°æ®
    documents = fetch_docs_from_qdrant()
    
    if not documents:
        print("âŒ æ•°æ®åº“ä¸ºç©ºï¼Œæ— æ³•ç”Ÿæˆè¯•é¢˜ï¼è¯·å…ˆè¿è¡Œ worker.py")
        return

    # 2. åˆå§‹åŒ–å‡ºé¢˜æ¨¡å‹ (å»ºè®®ç”¨æœ€å¼ºçš„æ¨¡å‹å‡ºé¢˜ï¼Œå¦‚ qwen-max)
    llm = ChatOpenAI(
        model="qwen-plus-2025-09-11", 
        api_key=os.getenv("DASHSCOPE_API_KEY"),
        temperature=0.0,
        base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
    )

    embeddings = DashScopeEmbeddings(
        dashscope_api_key=os.getenv("DASHSCOPE_API_KEY")
    )

    # 3. Ragas ç”Ÿæˆå™¨
    generator = TestsetGenerator.from_langchain(
        llm=llm,
        embedding_model=embeddings,
        llm_context=(
        "ä½ æ˜¯ä¸€ä¸ªä¿¡æ¯æŠ½å–æ¨¡å‹ï¼Œè€Œä¸æ˜¯è®²è§£æ¨¡å‹ã€‚\n"
        "âš ï¸ ä½ å¿…é¡»ä¸¥æ ¼ã€åªã€ä¸”ä»…è¾“å‡º JSONã€‚\n"
        "âš ï¸ ä¸å…è®¸è¾“å‡ºä»»ä½•è§£é‡Šæ€§æ–‡å­—ã€æ®µè½ã€è¯´æ˜ã€‚\n"
        "âš ï¸ ä¸å…è®¸å‡ºç°é JSON å†…å®¹ã€‚\n"
        "âš ï¸ JSON å¿…é¡»æ˜¯å•ä¸ªå¯¹è±¡ï¼Œè€Œä¸æ˜¯æ•°ç»„ã€‚\n"
        "æ‰€æœ‰å­—æ®µå€¼å¿…é¡»æ˜¯å­—ç¬¦ä¸²ã€‚\n"
        "æ‰€æœ‰å†…å®¹ä½¿ç”¨ã€ç®€ä½“ä¸­æ–‡ã€‘ã€‚\n"
        "âš ï¸ ä¸¥ç¦ä½¿ç”¨ LaTeX è¯­æ³•ï¼ˆå¦‚ \\( \\)ã€$ $ï¼‰ã€‚\n"
        "âš ï¸ ä¸¥ç¦åœ¨å­—ç¬¦ä¸²ä¸­å‡ºç°åæ–œæ  \\ ã€‚\n"
        "å¦‚æ¶‰åŠæ•°å­¦å…¬å¼ï¼Œè¯·ç”¨è‡ªç„¶è¯­è¨€æè¿°ï¼Œä¸è¦å†™å…¬å¼ã€‚\n"
        )
    )

    run_config = RunConfig(
        max_workers=3,
        max_retries=5,
        timeout=120
    )

    print("ğŸ§  æ­£åœ¨æ ¹æ®æ•°æ®åº“å†…å®¹ç”Ÿæˆè€ƒé¢˜ (è¿™éœ€è¦ä¸€ç‚¹æ—¶é—´)...")
    dataset = generator.generate_with_langchain_docs(
        documents,
        testset_size=TESTSET_SIZE,
        run_config=run_config,
    )

    # 4. å¯¼å‡º CSV
    df = dataset.to_pandas()
    
    # æ˜ å°„åˆ—åä»¥é€‚é…ä½ çš„ test.py
    if "user_input" in df.columns:
        df = df.rename(columns={"user_input": "question", "reference": "ground_truth"})
    
    # è¿™é‡Œçš„ ground_truth æ˜¯ Ragas ç”Ÿæˆçš„æ ‡å‡†ç­”æ¡ˆ
    # è¿™é‡Œçš„ question æ˜¯ Ragas ç”Ÿæˆçš„é—®é¢˜
    

    output_file = Path(__file__).resolve().parent / "my_golden_dataset.csv"
    df.to_csv(output_file, index=False)
    print(f"ğŸ‰ è€ƒå·ç”Ÿæˆå®Œæ¯•ï¼å·²ä¿å­˜è‡³ {output_file}")
    print(df[["question", "ground_truth"]].head(2))

if __name__ == "__main__":
    main()